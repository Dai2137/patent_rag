from pathlib import Path
import json
import pandas as pd
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile

# --- æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from infra.config import PROJECT_ROOT, PathManager, DirNames
from model.patent import Patent
from ui.gui import query_detail
from ui.gui import ai_judge_detail
from ui.gui.prior_art_detail import prior_art_detail
from bigquery.patent_lookup import get_full_patent_info_by_doc_numbers

# å®šæ•°
MAX_CHAR = 300
EXCLUDE_DIRS = {
    DirNames.UPLOADED, DirNames.TOPK, "temp", DirNames.QUERY, DirNames.KNOWLEDGE,
    "__pycache__", ".git", ".ipynb_checkpoints"
}

def reset_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–"""
    keys_to_reset = [
        "df_retrieved", "matched_chunk_markdowns", "reasons",
        "query", "retrieved_docs", "search_results_df",
        "ai_judge_results", "file_content", "project_dir",
        "current_doc_number", "uploaded_dir"
    ]
    for key in keys_to_reset:
        if key in st.session_state:
            del st.session_state[key]

def load_project_by_id(doc_number: str) -> bool:
    """
    ã€å…±é€šå‡¦ç†ã€‘æŒ‡å®šã•ã‚ŒãŸ doc_number ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€SessionStateã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã‚‚ã€æ—¢å­˜é¸æŠæ™‚ã‚‚ã€æœ€çµ‚çš„ã«ã“ã‚Œã‚’å‘¼ã¶ã“ã¨ã§çŠ¶æ…‹ã‚’å¾©å…ƒã™ã‚‹ã€‚
    """
    # 1. ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    reset_session_state()

    try:
        # --- A. åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆXML/Queryï¼‰ã®ãƒ­ãƒ¼ãƒ‰ ---
        uploaded_dir = PathManager.get_uploaded_query_path(doc_number)
        query_file = uploaded_dir / "uploaded_query.txt"

        if not query_file.exists():
            st.error(f"âŒ å‡ºé¡˜ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {query_file}")
            return False

        with open(query_file, "r", encoding="utf-8") as f:
            file_content = f.read()

        # XMLè§£æ
        query: Patent = st.session_state.loader.run(query_file)

        # åŸºæœ¬ã‚¹ãƒ†ãƒ¼ãƒˆè¨­å®š
        st.session_state.file_content = file_content
        st.session_state.query = query
        st.session_state.project_dir = uploaded_dir.parent
        st.session_state.uploaded_dir = uploaded_dir
        st.session_state.current_doc_number = doc_number

        # --- B. æ¤œç´¢çµæœï¼ˆCSVï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        topk_dir = PathManager.get_topk_results_path(doc_number)
        if topk_dir.exists():
            csv_files = sorted(topk_dir.glob("*.csv"))
            if csv_files:
                latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)
                search_results_df = pd.read_csv(latest_csv)
                st.session_state.search_results_df = search_results_df
                st.session_state.df_retrieved = search_results_df
                st.session_state.search_results_csv_path = str(latest_csv)

        # --- C. AIå¯©æŸ»çµæœï¼ˆJSONï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        ai_judge_dir = PathManager.get_ai_judge_result_path(doc_number)
        aj_judge_data_success = False
        if ai_judge_dir.exists():
            json_files = sorted(ai_judge_dir.glob("*.json"))
            if json_files:
                latest_json = json_files[-1]
                with open(latest_json, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                st.session_state.ai_judge_results = results

                if st.session_state.ai_judge_results:
                    aj_judge_data_success = True

        return True

    except Exception as e:
        st.error(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ {doc_number} ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.code(traceback.format_exc())
        return False

def handle_new_upload(uploaded_file: UploadedFile):
    """æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®å‡¦ç†ï¼šä¿å­˜ã—ã¦IDã‚’ç‰¹å®šã—ã€å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã¶"""
    try:
        file_content = uploaded_file.read().decode("utf-8")

        # 1. ä¸€æ™‚ä¿å­˜ã—ã¦IDè§£æ (doc_numberã‚’å–å¾—ã™ã‚‹ãŸã‚)
        temp_path = PathManager.get_temp_path("uploaded_query.txt")
        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(file_content)

        with st.spinner("XMLã‚’è§£æä¸­..."):
            query: Patent = st.session_state.loader.run(temp_path)
            doc_number = query.publication.doc_number

            if not doc_number:
                st.error("âŒ XMLã‹ã‚‰ç‰¹è¨±ç•ªå·(doc_number)ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

        # 2. æ­£è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•ãƒ»ä¿å­˜
        PathManager.move_to_permanent(temp_path, doc_number)

        # 3. å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ãƒ‰ (ã“ã‚Œã§æ—¢å­˜ãƒ•ãƒ­ãƒ¼ã¨åˆæµ)
        if load_project_by_id(doc_number):
            st.success(f"âœ… æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆãƒ»ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {doc_number}")

    except UnicodeDecodeError:
        st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚UTF-8å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def page_1():
    st.title("GENIAC-PRIZE prototype")
    st.subheader("æ±äº¬å¤§å­¦æ¾å°¾å²©æ²¢ç ”ç©¶å®¤ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£")

    mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "2. æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º"))

    # --- å…¥åŠ›ã‚¨ãƒªã‚¢ã®æç”» ---
    if mode == "1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.header("ğŸ“ æ–°è¦å‡ºé¡˜ã®å¯©æŸ»")
        uploaded_file = st.file_uploader("1. XMLå½¢å¼ã®å‡ºé¡˜ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xml", "txt"])

        if uploaded_file is not None:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å–å¾—
            uploaded_content = uploaded_file.getvalue().decode("utf-8")
            current_content = st.session_state.get("file_content")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒå¤‰ã‚ã£ãŸå ´åˆã€ã¾ãŸã¯åˆå›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å ´åˆã«å‡¦ç†ã‚’å®Ÿè¡Œ
            if current_content != uploaded_content:
                handle_new_upload(uploaded_file)
            else:
                # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿
                st.info(f"ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {st.session_state.get('current_doc_number')}")

    else: # æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º
        st.header("ğŸ“‚ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‚ç…§")

        eval_dir = PathManager.EVAL_DIR
        if eval_dir.exists():
            projects = [
                d.name for d in eval_dir.iterdir()
                if d.is_dir() and not d.name.startswith('.') and d.name not in EXCLUDE_DIRS
            ]
            projects.sort(reverse=True)

            col1, col2 = st.columns([3, 1])
            with col1:
                selected_doc = st.selectbox("å‡ºé¡˜IDã‚’é¸æŠã—ã¦ãã ã•ã„", projects)
            with col2:
                if st.button("èª­è¾¼", type="primary", width="stretch"):
                    if selected_doc:
                        with st.spinner("ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                            if load_project_by_id(selected_doc):
                                st.success(f"âœ… {selected_doc} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                            # else:
                            #     st.error(f"âŒ {selected_doc} ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")

    # --- å…±é€šãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢æç”» ---
    # ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    if "query" in st.session_state and st.session_state.get("current_doc_number"):
        st.markdown("---")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸºæœ¬æƒ…å ±
        with st.expander(f"ğŸ“„ å‡ºé¡˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª: {st.session_state.current_doc_number}"):
            st.text_area("ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«", st.session_state.get("file_content", ""), height=150)

        # Step 2ä»¥é™ã®å…±é€šãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        render_common_steps()


def render_common_steps():
    """
    Step 2ä»¥é™ã®å…±é€šå‡¦ç†
    ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã« st.session_state ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å‰æã§å‹•ä½œã™ã‚‹
    """

    # --- Step 2: é¡ä¼¼æ–‡çŒ®æ¤œç´¢ ---
    st.header("2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢")

    has_search_results = 'search_results_df' in st.session_state and st.session_state.search_results_df is not None

    if has_search_results:
        st.info(f"ï¿½ï¿½ æ¤œç´¢çµæœ: {len(st.session_state.search_results_df):,}ä»¶ å–å¾—æ¸ˆã¿")

        if st.button("ğŸ“‹ è©³ç´°ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º", key="goto_search_list"):
            if "æ¤œç´¢çµæœä¸€è¦§" in st.session_state.page_map:
                st.switch_page(st.session_state.page_map["æ¤œç´¢çµæœä¸€è¦§"])
            else:
                st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: æ¤œç´¢çµæœä¸€è¦§")
        if st.button("ğŸ”„ æ¤œç´¢ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_search"):
            query_detail.query_detail()
    else:
        st.write("Google Patents Public Dataã‚’ç”¨ã„ã¦é¡ä¼¼æ–‡çŒ®ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
        if st.button("æ¤œç´¢å®Ÿè¡Œ", type="primary", key="run_new_search"):
            query_detail.query_detail()

    # --- Step 3: AIå¯©æŸ» ---
    st.header("3. AIå¯©æŸ»")

    has_ai_results = 'ai_judge_results' in st.session_state and st.session_state.ai_judge_results

    if has_ai_results:
        # æœ‰åŠ¹ãªçµæœã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        valid_results = [r for r in st.session_state.ai_judge_results if r is not None and not (isinstance(r, dict) and 'error' in r)]

        if len(valid_results) == 0:
            st.warning("âš ï¸ AIå¯©æŸ»ã®çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ’¾ å¯©æŸ»çµæœ: {len(valid_results)}ä»¶ å–å¾—æ¸ˆã¿")

            with st.expander("å¯©æŸ»çµæœä¸€è¦§ã‚’é–‹ã", expanded=True):
                # DataFrameã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                df_data = []
                valid_indices = []  # æœ‰åŠ¹ãªçµæœã®å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜

                # ai_judge_resultsã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                if 'ai_judge_results' not in st.session_state or not st.session_state.ai_judge_results:
                    st.error("âŒ AIå¯©æŸ»çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    return

                display_idx = 1
                for idx, result in enumerate(st.session_state.ai_judge_results):
                    # result ãŒ None ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if result is None:
                        continue

                    # result ãŒè¾æ›¸å‹ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not isinstance(result, dict):
                        continue

                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ã‚¹ã‚­ãƒƒãƒ—
                    if 'error' in result:
                        continue

                    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’åˆ¤å®š
                    claim_rejected = False
                    if 'inventiveness' in result:
                        try:
                            for claim in result["inventiveness"]:
                                inventiveness = result["inventiveness"][claim]
                                inventive_bool = inventiveness.get('inventive', True)
                                if not inventive_bool:
                                    claim_rejected = True
                                    break
                        except Exception as e:
                            continue

                    # å…¬å ±ç•ªå·ã‚’å–å¾—
                    try:
                        reference_doc_num = result.get('prior_art_doc_number', f"Doc #{display_idx}")
                    except Exception as e:
                        continue

                    # DataFrameã®è¡Œãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    df_data.append({
                        'é †ä½': display_idx,
                        'å…¬å ±ç•ªå·': reference_doc_num,
                        'ç´ä»˜ãå€™è£œã®æœ‰ç„¡': 'æœ‰' if claim_rejected else 'ç„¡'
                    })

                    valid_indices.append(idx)
                    display_idx += 1

                # DataFrameã‚’ä½œæˆã—ã¦è¡¨ç¤º
                if df_data:
                    df = pd.DataFrame(df_data)

                    # ä¿å­˜ç”¨ã®DataFrameã‚’ä½œæˆï¼ˆç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’True/Falseã«å¤‰æ›ï¼‰
                    df_to_save = df.copy()
                    df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] = df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'].map({'æœ‰': True, 'ç„¡': False})

                    # DataFrameã‚’ä¿å­˜
                    doc_number = st.session_state.current_doc_number
                    save_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")
                    df_to_save.to_csv(save_path, index=False, encoding='utf-8-sig')

                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name='ai_judge_results.csv',
                        mime='text/csv',
                    )

                    # ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ã«å¿œã˜ã¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨
                    # 10è¡Œã‚’è¶…ãˆã‚‹å ´åˆã®ã¿å›ºå®šé«˜ã•ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã«ã™ã‚‹
                    use_scrollable = len(df_data) > 10
                    container = st.container(height=450) if use_scrollable else st.container()

                    with container:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
                        header_cols = st.columns([1, 3, 2, 2])
                        with header_cols[0]:
                            st.markdown("**é †ä½**")
                        with header_cols[1]:
                            st.markdown("**å…¬å ±ç•ªå·**")
                        with header_cols[2]:
                            st.markdown("**ç´ä»˜ãå€™è£œã®æœ‰ç„¡**")
                        with header_cols[3]:
                            st.markdown("**AIå¯©æŸ»ã®è©³ç´°è¡¨ç¤º**")

                        st.divider()

                        # ãƒ‡ãƒ¼ã‚¿è¡Œ
                        for i, row_data in enumerate(df_data):
                            idx = valid_indices[i]
                            cols = st.columns([1, 3, 2, 2])

                            with cols[0]:
                                st.write(row_data['é †ä½'])
                            with cols[1]:
                                st.write(row_data['å…¬å ±ç•ªå·'])
                            with cols[2]:
                                st.write(row_data['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'])
                            with cols[3]:
                                if st.button("è©³ç´°", key=f"ai_detail_{idx}", use_container_width=True):
                                    st.session_state.selected_prior_art_idx = idx
                                    if "å…ˆè¡ŒæŠ€è¡“è©³ç´°" in st.session_state.page_map:
                                        st.switch_page(st.session_state.page_map["å…ˆè¡ŒæŠ€è¡“è©³ç´°"])
                                    else:
                                        st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: å…ˆè¡ŒæŠ€è¡“è©³ç´°")

        if st.button("ğŸ”„ AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_ai_judge"):
             run_ai_judge()
    else:
        st.write("LLMã‚’æ´»ç”¨ã—ã€æ–°è¦æ€§ãƒ»é€²æ­©æ€§ã‚’å¯©æŸ»ã—ã¾ã™ã€‚")
        if st.button("AIå¯©æŸ»å®Ÿè¡Œ", type="primary", key="run_ai_judge_new"):
            if not has_search_results:
                st.warning("âš ï¸ å…ˆã«ã€Œ2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                run_ai_judge()

    # --- Step 4: åˆ¤æ–­æ ¹æ‹ å‡ºåŠ› ---
    st.header("4. åˆ¤æ–­æ ¹æ‹ å‡ºåŠ›")

    if not has_ai_results:
        st.write("âš ï¸ AIå¯©æŸ»ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
    else:
        ai_judge_results = st.session_state.ai_judge_results
        if not ai_judge_results or all(r is None or (isinstance(r, dict) and 'error' in r) for r in ai_judge_results):
            st.warning("âš ï¸ æœ‰åŠ¹ãªAIå¯©æŸ»çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
            return
        
        doc_numbers_to_fetch = generate_reasons(ai_judge_results)
        if doc_numbers_to_fetch is None or len(doc_numbers_to_fetch) == 0:
            return
        current_doc_number = str(st.session_state.current_doc_number)
        year_part = current_doc_number[:4]
        doc_digit_part = current_doc_number[4:]
        formatted_current_doc_number = f"{year_part}-{doc_digit_part}"

        st.write(f"âœ…ç‰¹é¡˜ {formatted_current_doc_number}ã«ç´ã¥ã{len(doc_numbers_to_fetch)}ä»¶ã®æ–‡çŒ®ãŒã‚ã‚Šã¾ã™ã€‚")

        doc_number_output_number_dict = {}
        for i, reference_doc_num in enumerate(doc_numbers_to_fetch):
            reference_doc_num = str(reference_doc_num)
            year_part = reference_doc_num[:4]
            doc_digit_part = reference_doc_num[4:]
            formatted_doc_number = f"{year_part}-{doc_digit_part}"
            output_doc_number = f"{i + 1} - ç‰¹é–‹ {formatted_doc_number}å·å…¬å ±"
            st.write(output_doc_number)
            doc_number_output_number_dict[reference_doc_num] = output_doc_number

        # markdownå½¢å¼ã§æ ¹æ‹ è¡¨ç¤º ç®‡æ¡æ›¸ãã§è¡¨ç¤ºdoc_numbers_to_fetchã®ä¸‹ã«æ ¹æ‹ ã‚’è¡¨ç¤ºã™ã‚‹
        # configã§evidence_exstractionãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ã—ã€å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        evidence_extraction_dir = PathManager.get_dir(
            st.session_state.current_doc_number,
            DirNames.EVIDENCE_EXTRACTION
        )

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        evidence_files = list(evidence_extraction_dir.glob("*.json"))
        if evidence_files:
            st.markdown("## ğŸ“‚ å‡ºé¡˜æ–‡çŒ®ã®åŸºæœ¬æƒ…å ±")
            # stã‹ã‚‰patentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
            patent = st.session_state.query
            # abstract, claimsã‚’å–å¾—ã—ã€ã€Œæ¦‚è¦ã€ã€ã€Œè«‹æ±‚é …ï¼‘ã€ãªã©ã‚’çµåˆã—ã¦é•·ã„æ–‡å­—åˆ—ã‚’ä½œæˆ
            abstract_text = patent.abstract if patent.abstract else "N/A"
            claims_text = "\n".join([f"è«‹æ±‚é … {i + 1}: {claim}" for i, claim in enumerate(patent.claims)]) if patent.claims else "N/A"
            long_markdown_text = f"### æ¦‚è¦\n{abstract_text}\n\n### è«‹æ±‚é …\n{claims_text}\n"
            st.text_area(
                label="å‡ºé¡˜ã®æ¦‚è¦ã¨è«‹æ±‚é …",
                value=long_markdown_text,
                height=300,
                disabled=True # ç·¨é›†ä¸å¯ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰ã«ã™ã‚‹
            )

            st.info(f"ğŸ“‚ å‚ç…§ç®‡æ‰€è¡¨ç¤º: {len(evidence_files)}ä»¶ã®å‚ç…§æ–‡çŒ®ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
            # doc_numberã¨è¡¨ç¤ºç”¨ã®ç•ªå·ã®è¾æ›¸
            for reference_doc_num in doc_number_output_number_dict.keys():
                st.markdown(f"### ğŸ“‘ {doc_number_output_number_dict[reference_doc_num]} ã®åˆ¤æ–­æ ¹æ‹ ")

                for evidence_file in evidence_files:
                    if reference_doc_num in evidence_file.name:
                        break
                else:
                    st.warning(f"âŒ å¯¾å¿œã™ã‚‹evidence_extractionãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {reference_doc_num}")
                    continue
                display_evidence_section(reference_doc_num, evidence_file)


        if st.button("æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ", type="primary"):
            with st.spinner("BigQueryã‹ã‚‰ç‰¹è¨±æƒ…å ±ã‚’å–å¾—ä¸­..."):
                get_full_patent_info_by_doc_numbers(doc_numbers_to_fetch, st.session_state.current_doc_number)

def normalize_text_for_search(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢ç”¨ã«æ­£è¦åŒ–ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãƒ»æ”¹è¡Œã‚’å‰Šé™¤ï¼‰

    Args:
        text: æ­£è¦åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        str: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not text:
        return ""

    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã€åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã€æ”¹è¡Œã€ã‚¿ãƒ–ã‚’å‰Šé™¤
    normalized = text.replace("ã€€", "").replace(" ", "").replace("\n", "").replace("\t", "")
    return normalized


def parse_paragraph_id_from_quote(source_paragraph_raw, doc_full_content, quote):
    """
    æ®µè½IDã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã¨æ®µè½ç•ªå·ã‚’å–å¾—ã™ã‚‹
    æ®µè½IDãŒä¸æ­£ãªå½¢å¼ã®å ´åˆã¯ã€quoteã®å†…å®¹ã§doc_full_contentã‚’æ¤œç´¢ã™ã‚‹

    Args:
        source_paragraph_raw: æ®µè½IDã®ç”Ÿã®æ–‡å­—åˆ—ï¼ˆä¾‹ï¼š'[best_mode_0121]' ã¾ãŸã¯ '[0168]'ï¼‰
        doc_full_content: doc_full_contentã®JSONè¾æ›¸
        quote: å¼•ç”¨æ–‡ï¼ˆå¿…é ˆï¼‰

    Returns:
        tuple: (paragraph_name, paragraph_number) ã¾ãŸã¯ Noneï¼ˆã‚¨ãƒ©ãƒ¼ã®å ´åˆï¼‰

    Examples:
        >>> parse_paragraph_id_from_quote("[best_mode_0121]", doc_content, quote)
        ("best_mode", 121)

        >>> parse_paragraph_id_from_quote("[0168]", doc_content, quote_text)
        ("best_mode", 165)  # quoteã®å†…å®¹ã§æ¤œç´¢ã—ãŸçµæœ
    """
    # "[best_mode_0121]" -> "best_mode_0121"
    source_paragraph_id = source_paragraph_raw.strip("[]")

    # "_"ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼šé€šå¸¸ã®å‡¦ç†
    if "_" in source_paragraph_id:
        try:
            paragraph_name, paragraph_number_str = source_paragraph_id.rsplit("_", 1)
            paragraph_number = int(paragraph_number_str)
            return (paragraph_name, paragraph_number)
        except (ValueError, AttributeError):
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ quote ã§æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            pass

    # "_"ãŒãªã„å ´åˆã€ã¾ãŸã¯é€šå¸¸ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆï¼šquoteã§æ¤œç´¢
    if quote:
        # quoteã‚’æ­£è¦åŒ–
        normalized_quote = normalize_text_for_search(quote)

        if not normalized_quote:
            return None

        # doc_full_contentã®å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢
        section_order = ["technical_field", "background_art", "disclosure", "best_mode"]

        for section_name in section_order:
            section_content = doc_full_content.get("description", {}).get(section_name)

            # disclosureã¯ãƒã‚¹ãƒˆã•ã‚ŒãŸè¾æ›¸ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—
            if isinstance(section_content, dict):
                continue

            if isinstance(section_content, list):
                for paragraph_index, paragraph_text in enumerate(section_content):
                    # æ®µè½ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–
                    normalized_paragraph = normalize_text_for_search(paragraph_text)

                    # å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    if normalized_quote in normalized_paragraph:
                        return (section_name, paragraph_index)

        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        return None

    # quoteã‚‚ãªã„å ´åˆ
    return None


def display_evidence_section(reference_doc_num, evidence_file):
    """
    è¨¼æ‹ ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å®šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç•ªå·ã«é–¢é€£ã™ã‚‹è¨¼æ‹ ã‚’æŠ½å‡ºã—ã€
    æ˜ç´°æ›¸ã®è©²å½“ç®‡æ‰€ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤ºã™ã‚‹

    Args:
        reference_doc_num: å‚ç…§å…ˆè¡ŒæŠ€è¡“æ–‡çŒ®ç•ªå·
        evidence_file: è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    paragraph_name_dict = {
        "technical_field": "ã€æŠ€è¡“åˆ†é‡ã€‘",
        "background_art": "ã€èƒŒæ™¯æŠ€è¡“ã€‘",
        "disclosure": "ã€ç™ºæ˜ã®æ¦‚è¦ã€‘",
        "best_mode": "ã€ç™ºæ˜ã‚’å®Ÿæ–½ã™ã‚‹ãŸã‚ã®å½¢æ…‹ã€‘"
    }

    current_doc_number = st.session_state.current_doc_number

    # doc_full_contentãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    doc_full_content_dir = PathManager.get_dir(current_doc_number, DirNames.DOC_FULL_CONTENT)
    doc_full_content_file = doc_full_content_dir / f"{reference_doc_num}.json"

    if not doc_full_content_file.exists():
        st.warning(f"âŒ doc_full_contentãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {doc_full_content_file}")
        return

    with open(doc_full_content_file, "r", encoding="utf-8") as f:
        doc_full_content = json.load(f)

    # è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with open(evidence_file, "r", encoding="utf-8") as f:
        evidence_data_list = json.load(f)

    # --- Step 1: å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ ---
    target_evidence_data = None

    # evidence_data_listãŒé…åˆ—ã®å ´åˆ
    if isinstance(evidence_data_list, list):
        target_evidence_data = next(
            (item for item in evidence_data_list if item.get("doc_number") == reference_doc_num),
            None
        )
    # å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
    elif isinstance(evidence_data_list, dict) and evidence_data_list.get("doc_number") == reference_doc_num:
        target_evidence_data = evidence_data_list

    if not target_evidence_data:
        st.info(f"ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç•ªå· `{reference_doc_num}` ã«ä¸€è‡´ã™ã‚‹è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # --- Step 2: å…¨è¨¼æ‹ ã‚’åé›†ã—ã¦paragraph_nameã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ– ---
    evidence_groups = {}  # {paragraph_name: [{"quote": ..., "explanation": ..., ...}, ...]}

    for item in target_evidence_data.get("evidence_items", []):
        citations = item.get("citations", [])
        claim_scope = item.get("claim_scope", "")

        for citation in citations:
            quote = citation.get("quote", "").strip()
            source_paragraph_raw = citation.get("source_paragraph", "")
            explanation = citation.get("proves", "")

            if not quote or not source_paragraph_raw:
                continue

            # æ–°ã—ã„é–¢æ•°ã‚’ä½¿ã£ã¦æ®µè½IDã‚’ãƒ‘ãƒ¼ã‚¹
            result = parse_paragraph_id_from_quote(source_paragraph_raw, doc_full_content, quote)

            if result is None:
                st.warning(f"âš ï¸ æ®µè½IDã®å½¢å¼ãŒä¸æ­£ã§ã™: `{source_paragraph_raw}` (è©²å½“ã™ã‚‹æ®µè½ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)")
                continue

            paragraph_name, paragraph_number = result
            paragraph_name_japanese = paragraph_name_dict.get(paragraph_name)

            if not paragraph_name_japanese:
                st.warning(f"âš ï¸ æœªå¯¾å¿œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³: `{paragraph_name}` (æ®µè½ID: {source_paragraph_raw})")
                continue

            # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if paragraph_name not in evidence_groups:
                evidence_groups[paragraph_name] = []

            evidence_groups[paragraph_name].append({
                "quote": quote,
                "explanation": explanation,
                "paragraph_number": paragraph_number,
                "source_paragraph_id": source_paragraph_raw.strip("[]"),
                "claim_scope": claim_scope
            })

    if not evidence_groups:
        st.info("ğŸ“ è¡¨ç¤ºå¯èƒ½ãªè¨¼æ‹ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # --- Step 3: ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è¨¼æ‹ è©³ç´°ã¨è©²å½“ç®‡æ‰€ã‚’è¡¨ç¤º ---
    for paragraph_name, evidence_list in sorted(evidence_groups.items()):
        paragraph_name_japanese = paragraph_name_dict[paragraph_name]

        # doc_full_contentã«è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‹ç¢ºèª
        if "description" not in doc_full_content or paragraph_name not in doc_full_content["description"]:
            st.warning(f"âš ï¸ æ˜ç´°æ›¸ãƒ‡ãƒ¼ã‚¿å†…ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ `{paragraph_name}` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            continue

        paragraph_list = doc_full_content["description"][paragraph_name]

        st.markdown(f"### ğŸ“„ {paragraph_name_japanese}")

        # å„è¨¼æ‹ ç•ªå·ã«å¯¾å¿œã™ã‚‹quoteã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
        paragraph_quotes = {}  # {paragraph_number: [(quote, claim_scope), ...]}

        for evidence in evidence_list:
            para_num = evidence["paragraph_number"]
            if para_num not in paragraph_quotes:
                paragraph_quotes[para_num] = []
            paragraph_quotes[para_num].append({
                "quote": evidence["quote"],
                "claim_scope": evidence["claim_scope"],
                "explanation": evidence["explanation"]
            })

        # å„è¨¼æ‹ ã®è©³ç´°ã‚’è¡¨ç¤º
        for idx, evidence in enumerate(evidence_list, 1):
            with st.expander(f"ğŸ” è¨¼æ‹  {idx}: {evidence['claim_scope']}", expanded=True):
                st.markdown(f"**ä¸€è‡´ç®‡æ‰€**")
                st.code(evidence['quote'], language=None)
                st.markdown(f"**ä¸€è‡´ã¨åˆ¤æ–­ã—ãŸç†ç”±**  \n{evidence['explanation']}")
                st.markdown(f"**ç®‡æ‰€**: æ˜ç´°æ›¸ {paragraph_name_japanese} **æ®µè½ {evidence['paragraph_number'] + 1}**")

        st.divider()

        # --- Step 4: è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å…¨æ®µè½ã‚’è¡¨ç¤ºï¼ˆè¤‡æ•°ã®quoteã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰ ---
        display_text_list = []

        for i in range(len(paragraph_list)):
            raw_paragraph = paragraph_list[i]

            # è©²å½“æ®µè½ã®å ´åˆï¼šè¤‡æ•°ã®quoteã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†
            if i in paragraph_quotes:
                clean_paragraph = raw_paragraph.replace("ã€€", "").replace(" ", "")

                # è¤‡æ•°ã®quoteã‚’ã™ã¹ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆé•·ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦éƒ¨åˆ†ä¸€è‡´ã‚’é˜²ãï¼‰
                quotes_sorted = sorted(
                    paragraph_quotes[i],
                    key=lambda x: len(x["quote"]),
                    reverse=True
                )

                for quote_info in quotes_sorted:
                    clean_quote = quote_info["quote"].replace("ã€€", "").replace(" ", "")
                    if clean_quote and clean_quote in clean_paragraph:
                        yellow_highlight = f"<mark style='background-color: #ffeb3b;'>{clean_quote}</mark>"
                        clean_paragraph = clean_paragraph.replace(clean_quote, yellow_highlight, 1)

                display_text_list.append(f"<b>ã€æ®µè½ {i+1}ã€‘</b> {clean_paragraph}")
            else:
                # é€šå¸¸ã®æ®µè½
                display_text_list.append(f"ã€æ®µè½ {i+1}ã€‘ {raw_paragraph}")

        # å…¨æ®µè½ã‚’çµåˆã—ã¦è¡¨ç¤º
        if display_text_list:
            full_context_text = "<br><br>".join(display_text_list)
        else:
            full_context_text = "âš ï¸ è¡¨ç¤ºå¯èƒ½ãªæ®µè½ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

        with st.container(height=400):
            st.markdown(
                f"**è©²å½“ç®‡æ‰€ã®å†…å®¹**  \næ˜ç´°æ›¸: {paragraph_name_japanese}  \n\n{full_context_text}",
                unsafe_allow_html=True
            )

        st.divider()

def run_ai_judge():
    """AIå¯©æŸ»å®Ÿè¡Œãƒ©ãƒƒãƒ‘ãƒ¼"""
    st.session_state.n_topk = len(st.session_state.df_retrieved)
    with st.spinner("å¯©æŸ»ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œä¸­..."):
        results = ai_judge_detail.entry(action="button_click")
        if results:
            st.session_state.ai_judge_results = results
            st.success("âœ… AIå¯©æŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()

def generate_reasons(ai_judge_results):
    """æ ¹æ‹ ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯"""
    # query_object = st.session_state.query
    # rejected_dfã‚’ï¼™ä»¶ã¾ã§è¡¨ç¤ºã™ã‚‹
    #ï¼™ä»¶ã«æº€ãŸãªã„å ´åˆã¯ã€top_kã‹ã‚‰ä¸è¶³ã—ã¦ã„ã‚‹åˆ†ã‚’è£œå®Œã™ã‚‹
    competition_rule_max_m = 9
    print(competition_rule_max_m, ": mMaxã®è¨­å®š")


    # eval/{doc_number}/ai_judge_result_tableã‹ã‚‰csvã‚’èª­ã¿è¾¼ã¿
    doc_number = st.session_state.current_doc_number
    csv_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")

    if not csv_path.exists():
        st.error(f"âŒ AIå¯©æŸ»çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return

    # CSVã‚’èª­ã¿è¾¼ã¿
    df_ai_judge = pd.read_csv(csv_path, encoding='utf-8-sig')

    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡_boolãŒTrueã®ã‚‚ã®ã‚’æŠ½å‡º
    rejected_df = df_ai_judge[df_ai_judge['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] == True].copy()

    if len(rejected_df) == 0:
        st.info("âœ… ç´ä»˜ãå€™è£œãŒã‚ã‚‹æ–‡çŒ®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # rejected_dfã«doc_numberåˆ—ã¨top_kåˆ—ã‚’è¿½åŠ 
    rejected_df['doc_number'] = rejected_df['å…¬å ±ç•ªå·']
    rejected_df['reject_document_exists'] = rejected_df['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool']

    actual_limit = min(competition_rule_max_m, len(rejected_df))

    # rejected_dfã‹ã‚‰å…¨ã¦ã®doc_numberã‚’å–å¾—
    doc_numbers_to_fetch = rejected_df.head(actual_limit)['doc_number'].tolist()
    reject_document_exists_list = rejected_df.head(actual_limit)['reject_document_exists'].tolist() 
    # reject_document_exists_listãŒTrueã®ã‚‚ã®ã ã‘ã«çµã‚‹
    doc_numbers_to_fetch = [doc_num for doc_num, exists in zip(doc_numbers_to_fetch, reject_document_exists_list) if exists]    
    return doc_numbers_to_fetch

    # # BigQueryã‹ã‚‰ä¸€æ‹¬ã§ç‰¹è¨±æƒ…å ±ã‚’å–å¾—
    # with st.spinner("BigQueryã‹ã‚‰ç‰¹è¨±æƒ…å ±ã‚’å–å¾—ä¸­..."):
    #     get_full_patent_info_by_doc_numbers(doc_numbers_to_fetch, doc_number)


    # # doc_numberã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã«å¤‰æ›ï¼ˆé«˜é€Ÿæ¤œç´¢ã®ãŸã‚ï¼‰
    # patent_info_dict = {info['doc_number']: info for info in patent_info_list}

    # # retrieved_docsã«ç‰¹è¨±æƒ…å ±ã‚’è¿½åŠ ã¾ãŸã¯æ›´æ–°
    # if "retrieved_docs" not in st.session_state:
    #     st.session_state.retrieved_docs = []

    # for i, target_row in rejected_df.head(actual_limit).iterrows():
    #     doc_number = target_row['doc_number']

    #     # å¯¾å¿œã™ã‚‹retrieved_docsã‚’æ¢ã™
    #     doc_found = False
    #     for doc in st.session_state.retrieved_docs:
    #         if doc.get('doc_number') == doc_number:
    #             # æ—¢å­˜ã®docã«BigQueryã‹ã‚‰å–å¾—ã—ãŸæƒ…å ±ã‚’è¿½åŠ 
    #             if doc_number in patent_info_dict:
    #                 patent_info = patent_info_dict[doc_number]
    #                 doc['title'] = patent_info['title']
    #                 doc['abstract'] = patent_info['abstract']
    #                 doc['claims'] = patent_info['claims']
    #                 doc['description'] = patent_info['description']
    #             doc_found = True
    #             break

    #     # è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ã€æ–°è¦ã«docã‚’ä½œæˆ
    #     if not doc_found and doc_number in patent_info_dict:
    #         patent_info = patent_info_dict[doc_number]
    #         new_doc = {
    #             'doc_number': doc_number,
    #             'title': patent_info['title'],
    #             'abstract': patent_info['abstract'],
    #             'claims': patent_info['claims'],
    #             'description': patent_info['description']
    #         }
    #         st.session_state.retrieved_docs.append(new_doc)

    # st.success(f"âœ… {len(patent_info_list)}ä»¶ã®ç‰¹è¨±æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")




    # st.session_state.reasons = []
    # status_text = st.empty()
    # progress = st.progress(0)
    # final_decision = ai_judge_results[0]["final_decision"] 
    # conversation_history = ai_judge_results[0]["conversation_history"] 
    # inventiveness_keys = dict(ai_judge_results[0]["inventiveness"]).keys()
    # for key in inventiveness_keys:
    #     if key.startswith('claim'):
    #         st.session_state.query.claims.append(key.upper())

    #  # å‹•ä½œç¢ºèªç”¨ãƒ€ãƒŸãƒ¼ã‚¢ã‚¯ã‚»ã‚¹
    # (['doc_number', 'top_k', 'application_structure', 'prior_art_structure', 'applicant_arguments', 'examiner_review', 'final_decision', 'conversation_history', 'inventiveness', 'prior_art_doc_number'])

    # for i in range(actual_limit):
    #     status_text.text(f"{i + 1} / {actual_limit} ä»¶ç›®ã‚’ç”Ÿæˆä¸­ã§ã™...")
    #     if "generator" in st.session_state:
    #         reason = st.session_state.generator.generate(
    #             st.session_state.query,
    #             st.session_state.retrieved_docs[i]
    #         )
    #         st.session_state.reasons.append(reason)
    #     else:
    #         st.error("GeneratorãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    #         break
    #     progress.progress((i + 1) / actual_limit)

    # status_text.text("ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    page_1()