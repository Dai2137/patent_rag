from pathlib import Path
import json
import pandas as pd
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile
from langchain_core.documents import Document   # â˜…è¿½åŠ 
import os
import re
import google.generativeai as genai
import html


# --- æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from infra.config import PROJECT_ROOT, PathManager, DirNames
from model.patent import Patent
from ui.gui import query_detail
from ui.gui import ai_judge_detail
from ui.gui.search_results_list import search_results_list
from ui.gui.prior_art_detail import prior_art_detail
from bigquery.patent_lookup import get_full_patent_info_by_doc_numbers

# å®šæ•°
MAX_CHAR = 300
EXCLUDE_DIRS = {
    DirNames.UPLOADED, DirNames.TOPK, "temp", DirNames.QUERY, DirNames.KNOWLEDGE,
    "__pycache__", ".git", ".ipynb_checkpoints"
}


def _normalize_text(x) -> str:
    """BigQuery ã‹ã‚‰è¿”ã£ã¦ãã‚‹ list / None ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    if x is None:
        return ""
    if isinstance(x, list):
        # æ®µè½ãƒªã‚¹ãƒˆãªã©ã‚’1ã¤ã®æ–‡å­—åˆ—ã«ã™ã‚‹
        return "\n".join(_normalize_text(e) for e in x)
    return str(x)



def reset_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–"""
    keys_to_reset = [
        "df_retrieved", "matched_chunk_markdowns", "reasons",
        "query", "retrieved_docs", "search_results_df",
        "ai_judge_results", "file_content", "project_dir",
        "current_doc_number", "uploaded_dir"
    ]
    for key in keys_to_reset:
        if key in st.session_state:
            del st.session_state[key]

def load_project_by_id(doc_number: str) -> bool:
    """
    ã€å…±é€šå‡¦ç†ã€‘æŒ‡å®šã•ã‚ŒãŸ doc_number ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€SessionStateã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã‚‚ã€æ—¢å­˜é¸æŠæ™‚ã‚‚ã€æœ€çµ‚çš„ã«ã“ã‚Œã‚’å‘¼ã¶ã“ã¨ã§çŠ¶æ…‹ã‚’å¾©å…ƒã™ã‚‹ã€‚
    """
    # 1. ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    reset_session_state()

    try:
        # --- A. åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆXML/Queryï¼‰ã®ãƒ­ãƒ¼ãƒ‰ ---
        uploaded_dir = PathManager.get_uploaded_query_path(doc_number)
        query_file = uploaded_dir / "uploaded_query.txt"

        if not query_file.exists():
            st.error(f"âŒ å‡ºé¡˜ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {query_file}")
            return False

        with open(query_file, "r", encoding="utf-8") as f:
            file_content = f.read()

        # XMLè§£æ
        query: Patent = st.session_state.loader.run(query_file)

        # åŸºæœ¬ã‚¹ãƒ†ãƒ¼ãƒˆè¨­å®š
        st.session_state.file_content = file_content
        st.session_state.query = query
        st.session_state.project_dir = uploaded_dir.parent
        st.session_state.uploaded_dir = uploaded_dir
        st.session_state.current_doc_number = doc_number

        # --- B. æ¤œç´¢çµæœï¼ˆCSVï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        topk_dir = PathManager.get_topk_results_path(doc_number)
        if topk_dir.exists():
            csv_files = sorted(topk_dir.glob("*.csv"))
            if csv_files:
                latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)
                search_results_df = pd.read_csv(latest_csv)
                st.session_state.search_results_df = search_results_df
                st.session_state.df_retrieved = search_results_df
                st.session_state.search_results_csv_path = str(latest_csv)

        # --- C. AIå¯©æŸ»çµæœï¼ˆJSONï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        ai_judge_dir = PathManager.get_ai_judge_result_path(doc_number)
        if ai_judge_dir.exists():
            json_files = sorted(ai_judge_dir.glob("*.json"))
            if json_files:
                latest_json = json_files[-1]
                with open(latest_json, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                st.session_state.ai_judge_results = results

        return True

    except Exception as e:
        st.error(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ {doc_number} ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.code(traceback.format_exc())
        return False

def handle_new_upload(uploaded_file: UploadedFile):
    """æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®å‡¦ç†ï¼šä¿å­˜ã—ã¦IDã‚’ç‰¹å®šã—ã€å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã¶"""
    try:
        file_content = uploaded_file.read().decode("utf-8")

        # 1. ä¸€æ™‚ä¿å­˜ã—ã¦IDè§£æ (doc_numberã‚’å–å¾—ã™ã‚‹ãŸã‚)
        temp_path = PathManager.get_temp_path("uploaded_query.txt")
        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(file_content)

        with st.spinner("XMLã‚’è§£æä¸­..."):
            query: Patent = st.session_state.loader.run(temp_path)
            doc_number = query.publication.doc_number

            if not doc_number:
                st.error("âŒ XMLã‹ã‚‰ç‰¹è¨±ç•ªå·(doc_number)ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

        # 2. æ­£è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•ãƒ»ä¿å­˜
        PathManager.move_to_permanent(temp_path, doc_number)

        # 3. å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ãƒ‰ (ã“ã‚Œã§æ—¢å­˜ãƒ•ãƒ­ãƒ¼ã¨åˆæµ)
        if load_project_by_id(doc_number):
            st.success(f"âœ… æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆãƒ»ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {doc_number}")

    except UnicodeDecodeError:
        st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚UTF-8å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def page_1():
    st.title("GENIAC-PRIZE prototype")
    st.subheader("æ±äº¬å¤§å­¦æ¾å°¾å²©æ²¢ç ”ç©¶å®¤ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£")

    mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "2. æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º"))

    # --- å…¥åŠ›ã‚¨ãƒªã‚¢ã®æç”» ---
    if mode == "1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.header("ğŸ“ æ–°è¦å‡ºé¡˜ã®å¯©æŸ»")
        uploaded_file = st.file_uploader("1. XMLå½¢å¼ã®å‡ºé¡˜ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xml", "txt"])

        if uploaded_file is not None:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒã€ç¾åœ¨ãƒ­ãƒ¼ãƒ‰ä¸­ã®ã‚‚ã®ã¨é•ã†å ´åˆã®ã¿å‡¦ç†
            # (Streamlitã®ãƒªãƒ­ãƒ¼ãƒ‰å¯¾ç­–)
            current_content = st.session_state.get("file_content")

            # ã¾ã èª­ã¿è¾¼ã‚“ã§ã„ãªã„ã€ã‚ã‚‹ã„ã¯å†…å®¹ãŒå¤‰ã‚ã£ãŸå ´åˆã«å®Ÿè¡Œ
            # æ³¨: uploaded_file.getvalue()ãªã©ã§æ¯”è¼ƒã™ã‚‹æ–¹æ³•ã‚‚ã‚ã‚‹ãŒã€
            # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«æ—¢å­˜stateã®æœ‰ç„¡ã§åˆ¤å®šã—ã€ãƒœã‚¿ãƒ³ãªã—ã§å³æ™‚ãƒ­ãƒ¼ãƒ‰ã•ã›ã‚‹æŒ™å‹•ã‚’ç¶­æŒ
            if not current_content:
                 handle_new_upload(uploaded_file)
            else:
                 # ã™ã§ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ãŸå ´åˆã®æ¤œçŸ¥ã¯
                 # file_uploaderã®keyã‚’å¤‰ãˆã‚‹ã‹ã€IDæ¯”è¼ƒãŒå¿…è¦ã ãŒã€ä»Šå›ã¯ç°¡æ˜“å®Ÿè£…ã¨ã™ã‚‹
                 st.info(f"ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {st.session_state.get('current_doc_number')}")

    else: # æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º
        st.header("ğŸ“‚ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‚ç…§")

        eval_dir = PathManager.EVAL_DIR
        if eval_dir.exists():
            projects = [
                d.name for d in eval_dir.iterdir()
                if d.is_dir() and not d.name.startswith('.') and d.name not in EXCLUDE_DIRS
            ]
            projects.sort(reverse=True)

            col1, col2 = st.columns([3, 1])
            with col1:
                selected_doc = st.selectbox("å‡ºé¡˜IDã‚’é¸æŠã—ã¦ãã ã•ã„", projects)
            with col2:
                if st.button("èª­è¾¼", type="primary", width="stretch"):
                    if selected_doc:
                        with st.spinner("ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                            if load_project_by_id(selected_doc):
                                st.success(f"âœ… {selected_doc} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # --- å…±é€šãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢æç”» ---
    # ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    if "query" in st.session_state and st.session_state.get("current_doc_number"):
        st.markdown("---")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸºæœ¬æƒ…å ±
        with st.expander(f"ğŸ“„ å‡ºé¡˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª: {st.session_state.current_doc_number}"):
            st.text_area("ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«", st.session_state.get("file_content", ""), height=150)

        # Step 2ä»¥é™ã®å…±é€šãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        render_common_steps()


def render_common_steps():
    """
    Step 2ä»¥é™ã®å…±é€šå‡¦ç†
    ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã« st.session_state ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å‰æã§å‹•ä½œã™ã‚‹
    """

    # --- Step 2: é¡ä¼¼æ–‡çŒ®æ¤œç´¢ ---
    st.header("2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢")

    has_search_results = 'search_results_df' in st.session_state and st.session_state.search_results_df is not None

    if has_search_results:
        st.info(f"ï¿½ï¿½ æ¤œç´¢çµæœ: {len(st.session_state.search_results_df):,}ä»¶ å–å¾—æ¸ˆã¿")

        if st.button("ğŸ“‹ è©³ç´°ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º", key="goto_search_list"):
            if "æ¤œç´¢çµæœä¸€è¦§" in st.session_state.page_map:
                st.switch_page(st.session_state.page_map["æ¤œç´¢çµæœä¸€è¦§"])
            else:
                st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: æ¤œç´¢çµæœä¸€è¦§")
        if st.button("ğŸ”„ æ¤œç´¢ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_search"):
            query_detail.query_detail()
    else:
        st.write("Google Patents Public Dataã‚’ç”¨ã„ã¦é¡ä¼¼æ–‡çŒ®ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
        if st.button("æ¤œç´¢å®Ÿè¡Œ", type="primary", key="run_new_search"):
            query_detail.query_detail()

    # --- Step 3: AIå¯©æŸ» ---
    st.header("3. AIå¯©æŸ»")

    has_ai_results = 'ai_judge_results' in st.session_state and st.session_state.ai_judge_results

    if has_ai_results:
        # æœ‰åŠ¹ãªçµæœã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        valid_results = [r for r in st.session_state.ai_judge_results if r is not None and not (isinstance(r, dict) and 'error' in r)]

        if len(valid_results) == 0:
            st.warning("âš ï¸ AIå¯©æŸ»ã®çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ’¾ å¯©æŸ»çµæœ: {len(valid_results)}ä»¶ å–å¾—æ¸ˆã¿")

            with st.expander("å¯©æŸ»çµæœä¸€è¦§ã‚’é–‹ã", expanded=True):
                # DataFrameã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                df_data = []
                valid_indices = []  # æœ‰åŠ¹ãªçµæœã®å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜

                display_idx = 1
                for idx, result in enumerate(st.session_state.ai_judge_results):
                    # result ãŒ None ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if result is None:
                        continue

                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ã‚¹ã‚­ãƒƒãƒ—
                    if isinstance(result, dict) and 'error' in result:
                        continue

                    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’åˆ¤å®š
                    claim_rejected = False
                    if 'inventiveness' in result:
                        for claim in result["inventiveness"]:
                            inventiveness = result["inventiveness"][claim]
                            inventive_bool = inventiveness.get('inventive', True)
                            if not inventive_bool:
                                claim_rejected = True
                                break

                    # å…¬å ±ç•ªå·ã‚’å–å¾—
                    doc_num = result.get('prior_art_doc_number', f"Doc #{display_idx}")

                    # DataFrameã®è¡Œãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    df_data.append({
                        'é †ä½': display_idx,
                        'å…¬å ±ç•ªå·': doc_num,
                        'ç´ä»˜ãå€™è£œã®æœ‰ç„¡': 'æœ‰' if claim_rejected else 'ç„¡'
                    })

                    valid_indices.append(idx)
                    display_idx += 1

                # DataFrameã‚’ä½œæˆã—ã¦è¡¨ç¤º
                if df_data:
                    df = pd.DataFrame(df_data)

                    # ä¿å­˜ç”¨ã®DataFrameã‚’ä½œæˆï¼ˆç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’True/Falseã«å¤‰æ›ï¼‰
                    df_to_save = df.copy()
                    df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] = df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'].map({'æœ‰': True, 'ç„¡': False})

                    # DataFrameã‚’ä¿å­˜
                    doc_number = st.session_state.current_doc_number
                    save_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")
                    df_to_save.to_csv(save_path, index=False, encoding='utf-8-sig')

                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name='ai_judge_results.csv',
                        mime='text/csv',
                    )

                    # ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ã«å¿œã˜ã¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨
                    # 10è¡Œã‚’è¶…ãˆã‚‹å ´åˆã®ã¿å›ºå®šé«˜ã•ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã«ã™ã‚‹
                    use_scrollable = len(df_data) > 10
                    container = st.container(height=450) if use_scrollable else st.container()

                    with container:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
                        header_cols = st.columns([1, 3, 2, 2])
                        with header_cols[0]:
                            st.markdown("**é †ä½**")
                        with header_cols[1]:
                            st.markdown("**å…¬å ±ç•ªå·**")
                        with header_cols[2]:
                            st.markdown("**ç´ä»˜ãå€™è£œã®æœ‰ç„¡**")
                        with header_cols[3]:
                            st.markdown("**AIå¯©æŸ»ã®è©³ç´°è¡¨ç¤º**")

                        st.divider()

                        # ãƒ‡ãƒ¼ã‚¿è¡Œ
                        for i, row_data in enumerate(df_data):
                            idx = valid_indices[i]
                            cols = st.columns([1, 3, 2, 2])

                            with cols[0]:
                                st.write(row_data['é †ä½'])
                            with cols[1]:
                                st.write(row_data['å…¬å ±ç•ªå·'])
                            with cols[2]:
                                st.write(row_data['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'])
                            with cols[3]:
                                if st.button("è©³ç´°", key=f"ai_detail_{idx}", use_container_width=True):
                                    st.session_state.selected_prior_art_idx = idx
                                    if "å…ˆè¡ŒæŠ€è¡“è©³ç´°" in st.session_state.page_map:
                                        st.switch_page(st.session_state.page_map["å…ˆè¡ŒæŠ€è¡“è©³ç´°"])
                                    else:
                                        st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: å…ˆè¡ŒæŠ€è¡“è©³ç´°")

        if st.button("ğŸ”„ AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_ai_judge"):
             run_ai_judge()
    else:
        st.write("LLMã‚’æ´»ç”¨ã—ã€æ–°è¦æ€§ãƒ»é€²æ­©æ€§ã‚’å¯©æŸ»ã—ã¾ã™ã€‚")
        if st.button("AIå¯©æŸ»å®Ÿè¡Œ", type="primary", key="run_ai_judge_new"):
            if not has_search_results:
                st.warning("âš ï¸ å…ˆã«ã€Œ2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                run_ai_judge()

    # --- Step 4: åˆ¤æ–­æ ¹æ‹ å‡ºåŠ› ---
    st.header("4. åˆ¤æ–­æ ¹æ‹ å‡ºåŠ›")

    if not has_ai_results:
        st.write("âš ï¸ AIå¯©æŸ»ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
    else:
        ai_judge_results = st.session_state.ai_judge_results
        if not ai_judge_results or all(r is None or (isinstance(r, dict) and 'error' in r) for r in ai_judge_results):
            st.warning("âš ï¸ æœ‰åŠ¹ãªAIå¯©æŸ»çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
            return
        
        doc_numbers_to_fetch = generate_reasons(ai_judge_results)
        if doc_numbers_to_fetch is None or len(doc_numbers_to_fetch) == 0:
            return
        current_doc_number = str(st.session_state.current_doc_number)
        year_part = current_doc_number[:4]
        doc_digit_part = current_doc_number[4:]
        formatted_current_doc_number = f"{year_part}-{doc_digit_part}"

        st.write(f"âœ…ç‰¹é¡˜ {formatted_current_doc_number}ã«ç´ã¥ã{len(doc_numbers_to_fetch)}ä»¶ã®æ–‡çŒ®ãŒã‚ã‚Šã¾ã™ã€‚")

        doc_number_output_number_dict = {}
        
        for i, doc_num in enumerate(doc_numbers_to_fetch):
            doc_num = str(doc_num)
            year_part = doc_num[:4]
            doc_digit_part = doc_num[4:]
            formatted_doc_number = f"{year_part}-{doc_digit_part}"
            output_doc_number = f"{i + 1} - ç‰¹é–‹ {formatted_doc_number}å·å…¬å ±"

            # æ–‡çŒ®ç•ªå·ã‚’è¡¨ç¤º
            st.write(output_doc_number)

            # æ–‡çŒ®ç•ªå· â†’ UIè¡¨ç¤ºå ã®è¾æ›¸
            doc_number_output_number_dict[doc_num] = output_doc_number

            # â˜… æ–‡çŒ®ã®ã™ãä¸‹ã«åˆ¤æ–­æ ¹æ‹ ã‚’è¡¨ç¤º
            if "reasons_by_doc" in st.session_state:
                reason = st.session_state.reasons_by_doc.get(doc_num)  # str(doc_num) ã§å–å¾—
                if reason:
                    st.markdown(f"#### ğŸ§  {output_doc_number} ã«å¯¾ã™ã‚‹åˆ¤æ–­æ ¹æ‹ ")
                    st.code(reason, language="markdown")
        

        # markdownå½¢å¼ã§æ ¹æ‹ è¡¨ç¤º ç®‡æ¡æ›¸ãã§è¡¨ç¤ºdoc_numbers_to_fetchã®ä¸‹ã«æ ¹æ‹ ã‚’è¡¨ç¤ºã™ã‚‹
        # configã§evidence_exstractionãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ã—ã€å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        evidence_extraction_dir = PathManager.get_dir(
            st.session_state.current_doc_number,
            DirNames.EVIDENCE_EXTRACTION
        )


        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        evidence_files = list(evidence_extraction_dir.glob("*.json"))
        if evidence_files:
            st.info(f"ğŸ“‚ å‚ç…§ç®‡æ‰€è¡¨ç¤º: {len(evidence_files)}ä»¶ã®å‚ç…§æ–‡çŒ®ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

            for doc_num, label in doc_number_output_number_dict.items():
                st.markdown(f"### ğŸ“‘ {label} ã®åˆ¤æ–­æ ¹æ‹ ")

                # doc_num ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘èª­ã‚€
                for evidence_file in evidence_files:
                    if str(doc_num) not in evidence_file.name:
                        continue

                    with open(evidence_file, "r", encoding="utf-8") as f:
                        evidence_data = json.load(f)

                    for item in evidence_data:
                        verified_evidence_list = item.get("verified_evidence", [])
                        for evidence_dict in verified_evidence_list:
                            claim_html = evidence_dict.get("claim_html")
                            prior_html = evidence_dict.get("prior_art_html")
                            reason_html = evidence_dict.get("reason_html") or evidence_dict.get("reason")

                            if claim_html:
                                st.markdown("**æœ¬é¡˜è«‹æ±‚é …ï¼ˆå¯¾å¿œé–¢ä¿‚ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰**")
                                st.markdown(claim_html, unsafe_allow_html=True)

                            if prior_html:
                                st.markdown("**å¼•ç”¨æ–‡çŒ®ï¼ˆå¯¾å¿œé–¢ä¿‚ã”ã¨ã«è‰²åˆ†ã‘ã—ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰**")
                                st.markdown(prior_html, unsafe_allow_html=True)

                            if reason_html:
                                st.markdown("**AIå¯©æŸ»ã®ç†ç”±**")
                                st.markdown(reason_html, unsafe_allow_html=True)

                            st.divider()




        if st.button("æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ", type="primary"):
            with st.spinner("LLM ã§æ ¹æ‹ ç®‡æ‰€ã‚’æŠ½å‡ºä¸­..."):
                query_patent: Patent = st.session_state.query
                ai_results = st.session_state.ai_judge_results

                run_evidence_extraction_for_doc_numbers(
                    query_patent=query_patent,
                    doc_numbers_to_fetch=doc_numbers_to_fetch,
                    ai_judge_results=ai_results,
                )

            st.success("âœ… æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            st.rerun()





        # if "reasons" in st.session_state and st.session_state.reasons:
        #     for i, reason in enumerate(st.session_state.reasons):
        #         st.markdown(f"##### åˆ¤æ–­æ ¹æ‹  {i + 1}")
        #         st.code(reason, language="markdown")


def run_ai_judge():
    """AIå¯©æŸ»å®Ÿè¡Œãƒ©ãƒƒãƒ‘ãƒ¼"""
    st.session_state.n_topk = len(st.session_state.df_retrieved)
    with st.spinner("å¯©æŸ»ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œä¸­..."):
        results = ai_judge_detail.entry(action="button_click")
        if results:
            st.session_state.ai_judge_results = results
            st.success("âœ… AIå¯©æŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()

def generate_reasons(ai_judge_results):
    """æ ¹æ‹ ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯"""
    # query_object = st.session_state.query
    # rejected_dfã‚’ï¼™ä»¶ã¾ã§è¡¨ç¤ºã™ã‚‹
    #ï¼™ä»¶ã«æº€ãŸãªã„å ´åˆã¯ã€top_kã‹ã‚‰ä¸è¶³ã—ã¦ã„ã‚‹åˆ†ã‚’è£œå®Œã™ã‚‹
    competition_rule_max_m = 9
    print(competition_rule_max_m, ": mMaxã®è¨­å®š")


    # eval/{doc_number}/ai_judge_result_tableã‹ã‚‰csvã‚’èª­ã¿è¾¼ã¿
    doc_number = st.session_state.current_doc_number
    csv_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")

    if not csv_path.exists():
        st.error(f"âŒ AIå¯©æŸ»çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return

    # CSVã‚’èª­ã¿è¾¼ã¿
    df_ai_judge = pd.read_csv(csv_path, encoding='utf-8-sig')

    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡_boolãŒTrueã®ã‚‚ã®ã‚’æŠ½å‡º
    rejected_df = df_ai_judge[df_ai_judge['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] == True].copy()

    if len(rejected_df) == 0:
        st.info("âœ… ç´ä»˜ãå€™è£œãŒã‚ã‚‹æ–‡çŒ®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # rejected_dfã«doc_numberåˆ—ã¨top_kåˆ—ã‚’è¿½åŠ 
    rejected_df['doc_number'] = rejected_df['å…¬å ±ç•ªå·']
    rejected_df['reject_document_exists'] = rejected_df['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool']

    actual_limit = min(competition_rule_max_m, len(rejected_df))

    # rejected_dfã‹ã‚‰å…¨ã¦ã®doc_numberã‚’å–å¾—
    doc_numbers_to_fetch = rejected_df.head(actual_limit)['doc_number'].tolist()
    reject_document_exists_list = rejected_df.head(actual_limit)['reject_document_exists'].tolist() 
    # reject_document_exists_listãŒTrueã®ã‚‚ã®ã ã‘ã«çµã‚‹
    doc_numbers_to_fetch = [doc_num for doc_num, exists in zip(doc_numbers_to_fetch, reject_document_exists_list) if exists]    
    return doc_numbers_to_fetch



def _highlight_snippets(text: str, snippets: list[str]) -> str:
    """snippets ã«å«ã¾ã‚Œã‚‹éƒ¨åˆ†æ–‡å­—åˆ—ã‚’ <mark> ã§1å›ãšã¤ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹"""
    # å¿µã®ãŸã‚ text ãŒ list ã®å ´åˆã‚‚å¯¾å¿œ
    if isinstance(text, list):
        text = "\n".join(_normalize_text(t) for t in text)

    if not text:
        return ""

    highlighted = str(text)
    for s in snippets:
        s = (s or "").strip()
        if not s:
            continue
        pattern = re.escape(s)
        highlighted = re.sub(
            pattern,
            lambda m: f"<mark>{m.group(0)}</mark>",
            highlighted,
            count=1,
        )
    return highlighted

def _build_highlighted_preview(text: str, snippet: str, marker: str, color: str, window: int = 50) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆä¸­ã® snippet ã®å‰å¾Œ window æ–‡å­—ã ã‘ã‚’æŠœãå‡ºã—ã€
    <mark style="background-color:...">marker + snippet</mark> ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ãŸçŸ­ã„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä½œã‚‹ã€‚
    """
    snippet = (snippet or "").strip()
    if not snippet:
        return ""

    text = _normalize_text(text)
    if not text:
        return ""

    idx = text.find(snippet)
    if idx == -1:
        # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° marker ä»˜ãã® snippet ã ã‘è¿”ã™
        escaped = html.escape(snippet)
        return f'<mark style="background-color:{color}; padding:0 2px; border-radius:3px;">{marker} {escaped}</mark>'

    start = max(0, idx - window)
    end = min(len(text), idx + len(snippet) + window)

    prefix = "â€¦" if start > 0 else ""
    suffix = "â€¦" if end < len(text) else ""

    before = html.escape(text[start:idx])
    target = html.escape(text[idx: idx + len(snippet)])
    after = html.escape(text[idx + len(snippet): end])

    return (
        f"{prefix}"
        f"{before}"
        f'<mark style="background-color:{color}; padding:0 2px; border-radius:3px;">{marker} {target}</mark>'
        f"{after}"
        f"{suffix}"
    )



def _extract_evidence_with_llm(
    claim_text: str,
    prior_art_text: str,
    reason_text: str | None = None,
) -> dict:
    """
    LLM ã«æ ¹æ‹ ãƒšã‚¢ã‚’ä½œã‚‰ã›ã€â‘ â‘¡â€¦ã®ãƒãƒ¼ã‚«ãƒ¼ä»˜ãã§
    ãƒã‚¤ãƒ©ã‚¤ãƒˆæ¸ˆã¿ HTML ï¼‹ ç†ç”±ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    """
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash",
        generation_config={"response_mime_type": "application/json"},
    )

    base_reason = reason_text or ""
    prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬ã®ç‰¹è¨±å¯©æŸ»å®˜ã§ã™ã€‚
ä»¥ä¸‹ã®æœ¬é¡˜è«‹æ±‚é …ãƒ†ã‚­ã‚¹ãƒˆã¨å…ˆè¡ŒæŠ€è¡“ãƒ†ã‚­ã‚¹ãƒˆã€ãŠã‚ˆã³ AI å¯©æŸ»ã§ã®ã€Œç™ºæ˜ã‚’å¦å®šã™ã‚‹ç†ç”±ã€ã«åŸºã¥ãã€
ã€Œç™ºæ˜ã‚’å¦å®šã™ã‚‹æ ¹æ‹ ã¨ãªã‚‹è«‹æ±‚é …ã®éƒ¨åˆ†ã€ã¨ã€Œå¯¾å¿œã™ã‚‹å…ˆè¡ŒæŠ€è¡“ã®éƒ¨åˆ†ã€ã®ãƒšã‚¢ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

[æœ¬é¡˜è«‹æ±‚é …å…¨æ–‡]
{claim_text}

[å…ˆè¡ŒæŠ€è¡“ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡]
{prior_art_text}

[AIå¯©æŸ»ã§ã®ç™ºæ˜å¦å®šã®ç†ç”±ï¼ˆå‚è€ƒï¼‰]
{base_reason}

å‡ºåŠ›ã¯å¿…ãš JSON å½¢å¼ã®ã¿ã¨ã—ã€æ¬¡ã®å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚

{{
  "evidence_pairs": [
    {{
      "claim_snippet": "æœ¬é¡˜è«‹æ±‚é …ã‹ã‚‰æŠœãå‡ºã—ãŸã€æ ¹æ‹ ã¨ãªã‚‹æ—¥æœ¬èªã®æ–‡ã¾ãŸã¯ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆåŸæ–‡ã‚’ãã®ã¾ã¾ï¼‰",
      "prior_art_snippet": "å…ˆè¡ŒæŠ€è¡“ã‹ã‚‰æŠœãå‡ºã—ãŸã€å¯¾å¿œã™ã‚‹æ—¥æœ¬èªã®æ–‡ã¾ãŸã¯ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆåŸæ–‡ã‚’ãã®ã¾ã¾ï¼‰",
      "explanation": "ãªãœã“ã®ãƒšã‚¢ãŒç™ºæ˜ã‚’å¦å®šã™ã‚‹æ ¹æ‹ ã«ãªã‚‹ã®ã‹ã‚’ç°¡æ½”ã«èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
    }},
    ...
  ]
}}

é‡è¦:
- snippet ã¯å¿…ãšä¸Šè¨˜ã® [æœ¬é¡˜è«‹æ±‚é …å…¨æ–‡] / [å…ˆè¡ŒæŠ€è¡“ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡] ã‹ã‚‰ãã®ã¾ã¾æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
- Markdown ã‚„ HTML ã‚¿ã‚° (<mark> ç­‰) ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""

    response = model.generate_content(prompt)
    try:
        data = json.loads(response.text)
        pairs = data.get("evidence_pairs", [])
    except Exception:
        pairs = []

    # â‘ ã€œâ‘© ã¨ã€ãã‚Œãã‚Œã®è‰²
    marker_chars = list("â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©")
    marker_colors = [
        "#fff59d",  # â‘ : é»„è‰²
        "#a5d6a7",  # â‘¡: ç·‘
        "#90caf9",  # â‘¢: é’
        "#ffccbc",  # â‘£: ã‚ªãƒ¬ãƒ³ã‚¸
        "#ce93d8",  # â‘¤: ç´«
        "#b0bec5",  # â‘¥: ã‚°ãƒ¬ãƒ¼
        "#ffe082",  # â‘¦: æ¿ƒã„ã‚é»„
        "#80cbc4",  # â‘§: é’ç·‘
        "#f48fb1",  # â‘¨: ãƒ”ãƒ³ã‚¯
        "#bcaaa4",  # â‘©: ãƒ–ãƒ©ã‚¦ãƒ³ç³»
    ]

    claim_previews: list[str] = []
    prior_previews: list[str] = []
    explanations_html: list[str] = []

    for idx, p in enumerate(pairs):
        marker = marker_chars[idx] if idx < len(marker_chars) else f"[{idx+1}]"
        color = marker_colors[idx % len(marker_colors)]

        c_snip = p.get("claim_snippet", "") or ""
        p_snip = p.get("prior_art_snippet", "") or ""
        expl   = p.get("explanation", "") or ""

        # æœ¬é¡˜ãƒ»å¼•ç”¨ãã‚Œãã‚Œã«ã¤ã„ã¦ã€Œå‘¨è¾º window æ–‡å­—ã ã‘ã€ã®æŠœç²‹ã‚’ä½œã‚‹ï¼ˆåŒã˜ marker & colorï¼‰
        c_preview = _build_highlighted_preview(claim_text, c_snip, marker, color, window=60)
        p_preview = _build_highlighted_preview(prior_art_text, p_snip, marker, color, window=60)

        if c_preview:
            claim_previews.append(c_preview)
        if p_preview:
            prior_previews.append(p_preview)

        if expl.strip():
            explanations_html.append(
                f'<p>'
                f'<mark style="background-color:{color}; padding:0 2px; border-radius:3px;">{marker}</mark> '
                f'{html.escape(expl.strip())}'
                f'</p>'
            )

    # <pre> ã§å›²ã‚“ã  HTML ã«ã™ã‚‹ï¼ˆè¤‡æ•°ãƒšã‚¢ã¯æ”¹è¡ŒåŒºåˆ‡ã‚Šã§ä¸¦ã¹ã‚‹ï¼‰
    claim_html = "<pre>" + "\n\n".join(claim_previews) + "</pre>" if claim_previews else ""
    prior_html = "<pre>" + "\n\n".join(prior_previews) + "</pre>" if prior_previews else ""

    # ç†ç”±ãƒ†ã‚­ã‚¹ãƒˆï¼šå…ƒã® reason_textï¼ˆã‚ã‚Œã°ï¼‰ï¼‹ è‰²ä»˜ã explanation ç¾¤
    base_reason = (reason_text or "").strip()
    if base_reason:
        base_reason_html = f"<p>{html.escape(base_reason)}</p>"
    else:
        base_reason_html = ""

    reason_html = base_reason_html + "".join(explanations_html)

    return {
        "claim_html": claim_html,
        "prior_art_html": prior_html,
        "reason_html": reason_html,
    }




def run_evidence_extraction_for_doc_numbers(
    query_patent: Patent,
    doc_numbers_to_fetch: list[str],
    ai_judge_results: list[dict],
):
    """
    - å¦å®šã•ã‚ŒãŸæ–‡çŒ® doc_number ã”ã¨ã«
      * æœ¬é¡˜è«‹æ±‚é …ãƒ†ã‚­ã‚¹ãƒˆ
      * prior_art ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‹è¦ç´„ï¼‹ã‚¯ãƒ¬ãƒ¼ãƒ ï¼‹æ˜ç´°æ›¸ï¼‰
      * AIå¯©æŸ»ã®ç†ç”±
      ã‚’ã¾ã¨ã‚ã¦ LLM ã«æŠ•ã’ã‚‹
    - çµæœã‚’ DirNames.EVIDENCE_EXTRACTION é…ä¸‹ã« JSON ã§ä¿å­˜
    """

    current_doc_number = str(query_patent.publication.doc_number)
    evidence_extraction_dir = PathManager.get_dir(
        current_doc_number,
        DirNames.EVIDENCE_EXTRACTION,
    )
    evidence_extraction_dir.mkdir(parents=True, exist_ok=True)

    # 1. BigQuery ã‹ã‚‰å…ˆè¡ŒæŠ€è¡“ã®æœ¬æ–‡ã‚’å–å¾—
    patent_infos = get_full_patent_info_by_doc_numbers(
        doc_numbers_to_fetch,
        current_doc_number=current_doc_number,
    )

    # doc_number -> prior_art_text
    prior_art_text_by_doc: dict[str, str] = {}
    for info in patent_infos:
        doc_num = str(info.get("doc_number", ""))
        if not doc_num:
            continue

        parts = [
            _normalize_text(info.get("invention_title") or info.get("title")),
            _normalize_text(info.get("abstract")),
            _normalize_text(info.get("claims")),
            _normalize_text(info.get("description")),
        ]
        prior_art_text_by_doc[doc_num] = "\n\n".join(p for p in parts if p.strip())

    # 2. AIå¯©æŸ»çµæœã‹ã‚‰ã€Œãã® doc_number ã§å¦å®šã•ã‚ŒãŸ claim ã®ç†ç”±ã€ã‚’ã¾ã¨ã‚ã‚‹
    reason_by_doc: dict[str, str] = {}
    for res in ai_judge_results:
        if not isinstance(res, dict):
            continue
        doc_num = str(res.get("prior_art_doc_number", ""))
        if not doc_num or doc_num not in doc_numbers_to_fetch:
            continue
        inv = res.get("inventiveness", {})
        reasons = []
        for claim_name, v in inv.items():
            if not isinstance(v, dict):
                continue
            if v.get("inventive", True) is False and v.get("reason"):
                reasons.append(f"{claim_name}: {v['reason']}")
        if reasons:
            reason_by_doc[doc_num] = "\n\n".join(reasons)

    # æœ¬é¡˜è«‹æ±‚é …ã¯ list ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã®ã§æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã‚‹
    if isinstance(query_patent.claims, list):
        claim_text = "\n".join(_normalize_text(c) for c in query_patent.claims)
    else:
        claim_text = _normalize_text(query_patent.claims)


    # 3. å„æ–‡çŒ®ã«ã¤ã„ã¦ LLM ã‚’å›ã—ã¦ JSON ã‚’ä¿å­˜
    for doc_num in doc_numbers_to_fetch:
        prior_text = prior_art_text_by_doc.get(str(doc_num), "")
        reason_text = reason_by_doc.get(str(doc_num), "")

        if not prior_text:
            # prior_art ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        evidence = _extract_evidence_with_llm(
            claim_text=claim_text,
            prior_art_text=prior_text,
            reason_text=reason_text,
        )

        output_obj = [
            {
                "doc_number": str(doc_num),
                "verified_evidence": [
                    evidence,  # {"claim_html", "prior_art_html", "reason"}
                ],
            }
        ]

        out_path = evidence_extraction_dir / f"evidence_{doc_num}.json"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(output_obj, f, ensure_ascii=False, indent=2)



if __name__ == "__main__":
    page_1()