from pathlib import Path
import json
import pandas as pd
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile
import os
import google.generativeai as genai
import re


# --- æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from infra.config import PROJECT_ROOT, PathManager, DirNames
from model.patent import Patent
from ui.gui import query_detail
from ui.gui import ai_judge_detail
from ui.gui.search_results_list import search_results_list
from ui.gui.prior_art_detail import prior_art_detail
from bigquery.patent_lookup import get_full_patent_info_by_doc_numbers

# å®šæ•°
MAX_CHAR = 300
EXCLUDE_DIRS = {
    DirNames.UPLOADED, DirNames.TOPK, "temp", DirNames.QUERY, DirNames.KNOWLEDGE,
    "__pycache__", ".git", ".ipynb_checkpoints"
}

def reset_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–"""
    keys_to_reset = [
        "df_retrieved", "matched_chunk_markdowns", "reasons",
        "query", "retrieved_docs", "search_results_df",
        "ai_judge_results", "file_content", "project_dir",
        "current_doc_number", "uploaded_dir"
    ]
    for key in keys_to_reset:
        if key in st.session_state:
            del st.session_state[key]

def load_project_by_id(doc_number: str) -> bool:
    """
    ã€å…±é€šå‡¦ç†ã€‘æŒ‡å®šã•ã‚ŒãŸ doc_number ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€SessionStateã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã‚‚ã€æ—¢å­˜é¸æŠæ™‚ã‚‚ã€æœ€çµ‚çš„ã«ã“ã‚Œã‚’å‘¼ã¶ã“ã¨ã§çŠ¶æ…‹ã‚’å¾©å…ƒã™ã‚‹ã€‚
    """
    # 1. ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    reset_session_state()

    try:
        # --- A. åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆXML/Queryï¼‰ã®ãƒ­ãƒ¼ãƒ‰ ---
        uploaded_dir = PathManager.get_uploaded_query_path(doc_number)
        query_file = uploaded_dir / "uploaded_query.txt"

        if not query_file.exists():
            st.error(f"âŒ å‡ºé¡˜ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {query_file}")
            return False

        with open(query_file, "r", encoding="utf-8") as f:
            file_content = f.read()

        # XMLè§£æ
        query: Patent = st.session_state.loader.run(query_file)

        # åŸºæœ¬ã‚¹ãƒ†ãƒ¼ãƒˆè¨­å®š
        st.session_state.file_content = file_content
        st.session_state.query = query
        st.session_state.project_dir = uploaded_dir.parent
        st.session_state.uploaded_dir = uploaded_dir
        st.session_state.current_doc_number = doc_number

        # --- B. æ¤œç´¢çµæœï¼ˆCSVï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        topk_dir = PathManager.get_topk_results_path(doc_number)
        if topk_dir.exists():
            csv_files = sorted(topk_dir.glob("*.csv"))
            if csv_files:
                latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)
                search_results_df = pd.read_csv(latest_csv)
                st.session_state.search_results_df = search_results_df
                st.session_state.df_retrieved = search_results_df
                st.session_state.search_results_csv_path = str(latest_csv)

        # --- C. AIå¯©æŸ»çµæœï¼ˆJSONï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        ai_judge_dir = PathManager.get_ai_judge_result_path(doc_number)
        if ai_judge_dir.exists():
            json_files = sorted(ai_judge_dir.glob("*.json"))
            if json_files:
                latest_json = json_files[-1]
                with open(latest_json, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                st.session_state.ai_judge_results = results

        return True

    except Exception as e:
        st.error(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ {doc_number} ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.code(traceback.format_exc())
        return False

def handle_new_upload(uploaded_file: UploadedFile):
    """æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®å‡¦ç†ï¼šä¿å­˜ã—ã¦IDã‚’ç‰¹å®šã—ã€å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã¶"""
    try:
        file_content = uploaded_file.read().decode("utf-8")

        # 1. ä¸€æ™‚ä¿å­˜ã—ã¦IDè§£æ (doc_numberã‚’å–å¾—ã™ã‚‹ãŸã‚)
        temp_path = PathManager.get_temp_path("uploaded_query.txt")
        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(file_content)

        with st.spinner("XMLã‚’è§£æä¸­..."):
            query: Patent = st.session_state.loader.run(temp_path)
            doc_number = query.publication.doc_number

            if not doc_number:
                st.error("âŒ XMLã‹ã‚‰ç‰¹è¨±ç•ªå·(doc_number)ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

        # 2. æ­£è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•ãƒ»ä¿å­˜
        PathManager.move_to_permanent(temp_path, doc_number)

        # 3. å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ãƒ‰ (ã“ã‚Œã§æ—¢å­˜ãƒ•ãƒ­ãƒ¼ã¨åˆæµ)
        if load_project_by_id(doc_number):
            st.success(f"âœ… æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆãƒ»ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {doc_number}")

    except UnicodeDecodeError:
        st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚UTF-8å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def page_1():
    st.title("GENIAC-PRIZE prototype")
    st.subheader("æ±äº¬å¤§å­¦æ¾å°¾å²©æ²¢ç ”ç©¶å®¤ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£")

    mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "2. æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º"))

    # --- å…¥åŠ›ã‚¨ãƒªã‚¢ã®æç”» ---
    if mode == "1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.header("ğŸ“ æ–°è¦å‡ºé¡˜ã®å¯©æŸ»")
        uploaded_file = st.file_uploader("1. XMLå½¢å¼ã®å‡ºé¡˜ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xml", "txt"])

        if uploaded_file is not None:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒã€ç¾åœ¨ãƒ­ãƒ¼ãƒ‰ä¸­ã®ã‚‚ã®ã¨é•ã†å ´åˆã®ã¿å‡¦ç†
            # (Streamlitã®ãƒªãƒ­ãƒ¼ãƒ‰å¯¾ç­–)
            current_content = st.session_state.get("file_content")

            # ã¾ã èª­ã¿è¾¼ã‚“ã§ã„ãªã„ã€ã‚ã‚‹ã„ã¯å†…å®¹ãŒå¤‰ã‚ã£ãŸå ´åˆã«å®Ÿè¡Œ
            # æ³¨: uploaded_file.getvalue()ãªã©ã§æ¯”è¼ƒã™ã‚‹æ–¹æ³•ã‚‚ã‚ã‚‹ãŒã€
            # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«æ—¢å­˜stateã®æœ‰ç„¡ã§åˆ¤å®šã—ã€ãƒœã‚¿ãƒ³ãªã—ã§å³æ™‚ãƒ­ãƒ¼ãƒ‰ã•ã›ã‚‹æŒ™å‹•ã‚’ç¶­æŒ
            if not current_content:
                 handle_new_upload(uploaded_file)
            else:
                 # ã™ã§ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ãŸå ´åˆã®æ¤œçŸ¥ã¯
                 # file_uploaderã®keyã‚’å¤‰ãˆã‚‹ã‹ã€IDæ¯”è¼ƒãŒå¿…è¦ã ãŒã€ä»Šå›ã¯ç°¡æ˜“å®Ÿè£…ã¨ã™ã‚‹
                 st.info(f"ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {st.session_state.get('current_doc_number')}")

    else: # æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º
        st.header("ğŸ“‚ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‚ç…§")

        eval_dir = PathManager.EVAL_DIR
        if eval_dir.exists():
            projects = [
                d.name for d in eval_dir.iterdir()
                if d.is_dir() and not d.name.startswith('.') and d.name not in EXCLUDE_DIRS
            ]
            projects.sort(reverse=True)

            col1, col2 = st.columns([3, 1])
            with col1:
                selected_doc = st.selectbox("å‡ºé¡˜IDã‚’é¸æŠã—ã¦ãã ã•ã„", projects)
            with col2:
                if st.button("èª­è¾¼", type="primary", width="stretch"):
                    if selected_doc:
                        with st.spinner("ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                            if load_project_by_id(selected_doc):
                                st.success(f"âœ… {selected_doc} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # --- å…±é€šãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢æç”» ---
    # ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    if "query" in st.session_state and st.session_state.get("current_doc_number"):
        st.markdown("---")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸºæœ¬æƒ…å ±
        with st.expander(f"ğŸ“„ å‡ºé¡˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª: {st.session_state.current_doc_number}"):
            st.text_area("ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«", st.session_state.get("file_content", ""), height=150)

        # Step 2ä»¥é™ã®å…±é€šãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        render_common_steps()


def render_common_steps():
    """
    Step 2ä»¥é™ã®å…±é€šå‡¦ç†
    ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã« st.session_state ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å‰æã§å‹•ä½œã™ã‚‹
    """

    # --- Step 2: é¡ä¼¼æ–‡çŒ®æ¤œç´¢ ---
    st.header("2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢")

    has_search_results = 'search_results_df' in st.session_state and st.session_state.search_results_df is not None

    if has_search_results:
        st.info(f"ï¿½ï¿½ æ¤œç´¢çµæœ: {len(st.session_state.search_results_df):,}ä»¶ å–å¾—æ¸ˆã¿")

        if st.button("ğŸ“‹ è©³ç´°ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º", key="goto_search_list"):
            if "æ¤œç´¢çµæœä¸€è¦§" in st.session_state.page_map:
                st.switch_page(st.session_state.page_map["æ¤œç´¢çµæœä¸€è¦§"])
            else:
                st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: æ¤œç´¢çµæœä¸€è¦§")
        if st.button("ğŸ”„ æ¤œç´¢ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_search"):
            query_detail.query_detail()
    else:
        st.write("Google Patents Public Dataã‚’ç”¨ã„ã¦é¡ä¼¼æ–‡çŒ®ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
        if st.button("æ¤œç´¢å®Ÿè¡Œ", type="primary", key="run_new_search"):
            query_detail.query_detail()

    # --- Step 3: AIå¯©æŸ» ---
    st.header("3. AIå¯©æŸ»")

    has_ai_results = 'ai_judge_results' in st.session_state and st.session_state.ai_judge_results

    if has_ai_results:
        # æœ‰åŠ¹ãªçµæœã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        valid_results = [r for r in st.session_state.ai_judge_results if r is not None and not (isinstance(r, dict) and 'error' in r)]

        if len(valid_results) == 0:
            st.warning("âš ï¸ AIå¯©æŸ»ã®çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ’¾ å¯©æŸ»çµæœ: {len(valid_results)}ä»¶ å–å¾—æ¸ˆã¿")

            with st.expander("å¯©æŸ»çµæœä¸€è¦§ã‚’é–‹ã", expanded=True):
                # DataFrameã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                df_data = []
                valid_indices = []  # æœ‰åŠ¹ãªçµæœã®å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜

                display_idx = 1
                for idx, result in enumerate(st.session_state.ai_judge_results):
                    # result ãŒ None ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if result is None:
                        continue

                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ã‚¹ã‚­ãƒƒãƒ—
                    if isinstance(result, dict) and 'error' in result:
                        continue

                    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’åˆ¤å®š
                    claim_rejected = False
                    if 'inventiveness' in result:
                        for claim in result["inventiveness"]:
                            inventiveness = result["inventiveness"][claim]
                            inventive_bool = inventiveness.get('inventive', True)
                            if not inventive_bool:
                                claim_rejected = True
                                break

                    # å…¬å ±ç•ªå·ã‚’å–å¾—
                    doc_num = result.get('prior_art_doc_number', f"Doc #{display_idx}")

                    # DataFrameã®è¡Œãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    df_data.append({
                        'é †ä½': display_idx,
                        'å…¬å ±ç•ªå·': doc_num,
                        'ç´ä»˜ãå€™è£œã®æœ‰ç„¡': 'æœ‰' if claim_rejected else 'ç„¡'
                    })

                    valid_indices.append(idx)
                    display_idx += 1

                # DataFrameã‚’ä½œæˆã—ã¦è¡¨ç¤º
                if df_data:
                    df = pd.DataFrame(df_data)

                    # ä¿å­˜ç”¨ã®DataFrameã‚’ä½œæˆï¼ˆç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’True/Falseã«å¤‰æ›ï¼‰
                    df_to_save = df.copy()
                    df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] = df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'].map({'æœ‰': True, 'ç„¡': False})

                    # DataFrameã‚’ä¿å­˜
                    doc_number = st.session_state.current_doc_number
                    save_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")
                    df_to_save.to_csv(save_path, index=False, encoding='utf-8-sig')

                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name='ai_judge_results.csv',
                        mime='text/csv',
                    )

                    # ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ã«å¿œã˜ã¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨
                    # 10è¡Œã‚’è¶…ãˆã‚‹å ´åˆã®ã¿å›ºå®šé«˜ã•ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã«ã™ã‚‹
                    use_scrollable = len(df_data) > 10
                    container = st.container(height=450) if use_scrollable else st.container()

                    with container:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
                        header_cols = st.columns([1, 3, 2, 2])
                        with header_cols[0]:
                            st.markdown("**é †ä½**")
                        with header_cols[1]:
                            st.markdown("**å…¬å ±ç•ªå·**")
                        with header_cols[2]:
                            st.markdown("**ç´ä»˜ãå€™è£œã®æœ‰ç„¡**")
                        with header_cols[3]:
                            st.markdown("**AIå¯©æŸ»ã®è©³ç´°è¡¨ç¤º**")

                        st.divider()

                        # ãƒ‡ãƒ¼ã‚¿è¡Œ
                        for i, row_data in enumerate(df_data):
                            idx = valid_indices[i]
                            cols = st.columns([1, 3, 2, 2])

                            with cols[0]:
                                st.write(row_data['é †ä½'])
                            with cols[1]:
                                st.write(row_data['å…¬å ±ç•ªå·'])
                            with cols[2]:
                                st.write(row_data['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'])
                            with cols[3]:
                                if st.button("è©³ç´°", key=f"ai_detail_{idx}", use_container_width=True):
                                    st.session_state.selected_prior_art_idx = idx
                                    if "å…ˆè¡ŒæŠ€è¡“è©³ç´°" in st.session_state.page_map:
                                        st.switch_page(st.session_state.page_map["å…ˆè¡ŒæŠ€è¡“è©³ç´°"])
                                    else:
                                        st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: å…ˆè¡ŒæŠ€è¡“è©³ç´°")

        if st.button("ğŸ”„ AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_ai_judge"):
             run_ai_judge()
    else:
        st.write("LLMã‚’æ´»ç”¨ã—ã€æ–°è¦æ€§ãƒ»é€²æ­©æ€§ã‚’å¯©æŸ»ã—ã¾ã™ã€‚")
        if st.button("AIå¯©æŸ»å®Ÿè¡Œ", type="primary", key="run_ai_judge_new"):
            if not has_search_results:
                st.warning("âš ï¸ å…ˆã«ã€Œ2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                run_ai_judge()

    # --- Step 4: åˆ¤æ–­æ ¹æ‹ å‡ºåŠ› ---
    st.header("4. åˆ¤æ–­æ ¹æ‹ å‡ºåŠ›")

    if not has_ai_results:
        st.write("âš ï¸ AIå¯©æŸ»ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
    else:
        ai_judge_results = st.session_state.ai_judge_results
        if not ai_judge_results or all(r is None or (isinstance(r, dict) and 'error' in r) for r in ai_judge_results):
            st.warning("âš ï¸ æœ‰åŠ¹ãªAIå¯©æŸ»çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
            return
        
        doc_numbers_to_fetch = generate_reasons(ai_judge_results)
        if doc_numbers_to_fetch is None or len(doc_numbers_to_fetch) == 0:
            return
        current_doc_number = str(st.session_state.current_doc_number)
        year_part = current_doc_number[:4]
        doc_digit_part = current_doc_number[4:]
        formatted_current_doc_number = f"{year_part}-{doc_digit_part}"

        st.write(f"âœ…ç‰¹é¡˜ {formatted_current_doc_number}ã«ç´ã¥ã{len(doc_numbers_to_fetch)}ä»¶ã®æ–‡çŒ®ãŒã‚ã‚Šã¾ã™ã€‚")

        doc_number_output_number_dict = {}
        for i, doc_num in enumerate(doc_numbers_to_fetch):
            doc_num = str(doc_num)
            year_part = doc_num[:4]
            doc_digit_part = doc_num[4:]
            formatted_doc_number = f"{year_part}-{doc_digit_part}"
            output_doc_number = f"{i + 1} - ç‰¹é–‹ {formatted_doc_number}å·å…¬å ±"
            st.write(output_doc_number)
            doc_number_output_number_dict[doc_num] = output_doc_number

        # markdownå½¢å¼ã§æ ¹æ‹ è¡¨ç¤º ç®‡æ¡æ›¸ãã§è¡¨ç¤ºdoc_numbers_to_fetchã®ä¸‹ã«æ ¹æ‹ ã‚’è¡¨ç¤ºã™ã‚‹
        # configã§evidence_exstractionãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—ã—ã€å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        evidence_extraction_dir = PathManager.get_dir(
            st.session_state.current_doc_number,
            DirNames.EVIDENCE_EXTRACTION
        )

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        evidence_files = list(evidence_extraction_dir.glob("*.json"))
        if evidence_files:
            st.info(f"ğŸ“‚ å‚ç…§ç®‡æ‰€è¡¨ç¤º: {len(evidence_files)}ä»¶ã®å‚ç…§æ–‡çŒ®ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

            for doc_num in doc_number_output_number_dict.keys():
                st.markdown(f"### ğŸ“‘ {doc_number_output_number_dict[doc_num]} ã®åˆ¤æ–­æ ¹æ‹ ")
                # listã®è¦ç´ ã®æ–‡å­—åˆ—ã«doc_numãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’æŠ½å‡º
                for evidence_file in evidence_files:
                    if doc_num in evidence_file.name:
                        with open(evidence_file, "r", encoding="utf-8") as f:
                            evidence_data = json.load(f)
                        for item in evidence_data:
                            verified_evidence_list = item["verified_evidence"]
                            for evidence_dict in verified_evidence_list:
                                for evidence_key in evidence_dict:
                                    evidence_content = evidence_dict[evidence_key]
                                    st.markdown(f"-   {evidence_key}:{evidence_content}")
                                st.divider()



        if st.button("æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ", type="primary"):
            with st.spinner("LLM ã§æ ¹æ‹ ç®‡æ‰€ã‚’æŠ½å‡ºä¸­..."):
                query_patent: Patent = st.session_state.query
                ai_results = st.session_state.ai_judge_results
                run_evidence_extraction_for_doc_numbers(
                    query_patent=query_patent,
                    doc_numbers_to_fetch=doc_numbers_to_fetch,
                    ai_judge_results=ai_results,
                )

            st.success("âœ… æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            st.rerun()
        
        
        evidence_files = list(evidence_extraction_dir.glob("*.json"))
        if evidence_files:
            st.info(f"ğŸ“‚ å‚ç…§ç®‡æ‰€è¡¨ç¤º: {len(evidence_files)}ä»¶ã®å‚ç…§æ–‡çŒ®ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")

            for doc_num in doc_number_output_number_dict.keys():
                st.markdown(f"### ğŸ“‘ {doc_number_output_number_dict[doc_num]} ã®åˆ¤æ–­æ ¹æ‹ ")

                for evidence_file in evidence_files:
                    if doc_num in evidence_file.name:
                        with open(evidence_file, "r", encoding="utf-8") as f:
                            evidence_data = json.load(f)

                        for item in evidence_data:
                            verified_evidence_list = item["verified_evidence"]
                            for evidence_dict in verified_evidence_list:
                                claim_html = evidence_dict.get("claim_html")
                                prior_html = evidence_dict.get("prior_art_html")
                                reason = evidence_dict.get("reason")

                                if claim_html:
                                    st.markdown("**è«‹æ±‚é …ï¼ˆæ ¹æ‹ ç®‡æ‰€ã‚’é»„è‰²ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰**")
                                    st.markdown(claim_html, unsafe_allow_html=True)

                                if prior_html:
                                    st.markdown("**å¼•ç”¨æ–‡çŒ®ï¼ˆæ ¹æ‹ ç®‡æ‰€ã‚’é»„è‰²ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰**")
                                    st.markdown(prior_html, unsafe_allow_html=True)

                                if reason:
                                    st.markdown("**AIå¯©æŸ»ã®ç†ç”±**")
                                    st.write(reason)

                                st.divider()


        # if "reasons" in st.session_state and st.session_state.reasons:
        #     for i, reason in enumerate(st.session_state.reasons):
        #         st.markdown(f"##### åˆ¤æ–­æ ¹æ‹  {i + 1}")
        #         st.code(reason, language="markdown")


def run_ai_judge():
    """AIå¯©æŸ»å®Ÿè¡Œãƒ©ãƒƒãƒ‘ãƒ¼"""
    st.session_state.n_topk = len(st.session_state.df_retrieved)
    with st.spinner("å¯©æŸ»ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œä¸­..."):
        results = ai_judge_detail.entry(action="button_click")
        if results:
            st.session_state.ai_judge_results = results
            st.success("âœ… AIå¯©æŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()

def generate_reasons(ai_judge_results):
    """æ ¹æ‹ ç”Ÿæˆç”¨ã«ã€AIå¯©æŸ»çµæœã‹ã‚‰ã€Œç´ä»˜ãå€™è£œã‚ã‚Šã€ã® doc_number ã‚’é›†ã‚ã¦è¿”ã™ã ã‘ã«ã™ã‚‹ã€‚

    - AIå¯©æŸ»çµæœ(ai_judge_results)ã‹ã‚‰ã€Œå°‘ãªãã¨ã‚‚1ã¤ã®è«‹æ±‚é …ã§ inventive=Falseã€ã«ãªã£ãŸ
      å…ˆè¡ŒæŠ€è¡“ã® doc_number ã‚’æŠ½å‡ºã™ã‚‹
    - é¡ä¼¼æ–‡çŒ®æ¤œç´¢çµæœ(df_retrieved)ã«ã¯ä¾å­˜ã—ãªã„
      ï¼ˆå…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã¯ BigQuery ã® get_full_patent_info_by_doc_numbers ã‹ã‚‰å–å¾—ã™ã‚‹ï¼‰
    """

    rejected_doc_numbers: list[str] = []

    for res in ai_judge_results:
        if not isinstance(res, dict):
            continue
        if res.get("error"):
            continue

        inventiveness = res.get("inventiveness")
        if not isinstance(inventiveness, dict):
            continue

        # å°‘ãªãã¨ã‚‚1ã¤ã®è«‹æ±‚é …ã§ã€Œinventive=Falseã€ãªã‚‰ç´ä»˜ãå€™è£œã¨ã¿ãªã™
        claim_rejected = any(
            isinstance(v, dict) and (v.get("inventive") is False)
            for v in inventiveness.values()
        )
        if not claim_rejected:
            continue

        # å…ˆè¡ŒæŠ€è¡“ã®ç•ªå·ã‚’å–å¾—
        doc_num = res.get("prior_art_doc_number") or res.get("doc_number")
        if not doc_num:
            continue

        doc_num = str(doc_num)
        rejected_doc_numbers.append(doc_num)

    if not rejected_doc_numbers:
        st.info("âœ… ç´ä»˜ãå€™è£œãŒã‚ã‚‹æ–‡çŒ®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    # é‡è¤‡å‰Šé™¤ã—ã¤ã¤é †åºç¶­æŒ
    unique_doc_numbers = list(dict.fromkeys(rejected_doc_numbers))
    return unique_doc_numbers


def _highlight_snippets(text: str, snippets: list[str]) -> str:
    """snippets ã«å«ã¾ã‚Œã‚‹éƒ¨åˆ†æ–‡å­—åˆ—ã‚’ <mark> ã§1å›ãšã¤ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹"""
    if not text:
        return ""

    highlighted = text
    for s in snippets:
        s = s.strip()
        if not s:
            continue
        # æ­£è¦è¡¨ç¾ã§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ã€æœ€åˆã®1å›ã ã‘ç½®æ›
        pattern = re.escape(s)
        highlighted = re.sub(
            pattern,
            lambda m: f"<mark>{m.group(0)}</mark>",
            highlighted,
            count=1,
        )
    return highlighted


def _extract_evidence_with_llm(
    claim_text: str,
    prior_art_text: str,
    reason_text: str | None = None,
) -> dict:
    """
    LLM ã«æ ¹æ‹ ãƒšã‚¢ã‚’ä½œã‚‰ã›ã€ãƒã‚¤ãƒ©ã‚¤ãƒˆæ¸ˆã¿ HTML ã‚’è¿”ã™

    æˆ»ã‚Šå€¤:
        {
            "claim_html": "<pre>...</pre>",
            "prior_art_html": "<pre>...</pre>",
            "reason": "ãƒ»ãƒ»ãƒ»"
        }
    """
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash",
        generation_config={"response_mime_type": "application/json"},
    )

    base_reason = reason_text or ""
    prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬ã®ç‰¹è¨±å¯©æŸ»å®˜ã§ã™ã€‚
ä»¥ä¸‹ã®æœ¬é¡˜è«‹æ±‚é …ãƒ†ã‚­ã‚¹ãƒˆã¨å…ˆè¡ŒæŠ€è¡“ãƒ†ã‚­ã‚¹ãƒˆã€ãŠã‚ˆã³ AI å¯©æŸ»ã§ã®ã€Œç™ºæ˜ã‚’å¦å®šã™ã‚‹ç†ç”±ã€ã«åŸºã¥ãã€
ã€Œç™ºæ˜ã‚’å¦å®šã™ã‚‹æ ¹æ‹ ã¨ãªã‚‹è«‹æ±‚é …ã®éƒ¨åˆ†ã€ã¨ã€Œå¯¾å¿œã™ã‚‹å…ˆè¡ŒæŠ€è¡“ã®éƒ¨åˆ†ã€ã®ãƒšã‚¢ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

[æœ¬é¡˜è«‹æ±‚é …å…¨æ–‡]
{claim_text}

[å…ˆè¡ŒæŠ€è¡“ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡]
{prior_art_text}

[AIå¯©æŸ»ã§ã®ç™ºæ˜å¦å®šã®ç†ç”±ï¼ˆå‚è€ƒï¼‰]
{base_reason}

å‡ºåŠ›ã¯å¿…ãš JSON å½¢å¼ã®ã¿ã¨ã—ã€æ¬¡ã®å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚

{{
  "evidence_pairs": [
    {{
      "claim_snippet": "æœ¬é¡˜è«‹æ±‚é …ã‹ã‚‰æŠœãå‡ºã—ãŸã€æ ¹æ‹ ã¨ãªã‚‹æ—¥æœ¬èªã®æ–‡ã¾ãŸã¯ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆåŸæ–‡ã‚’ãã®ã¾ã¾ï¼‰",
      "prior_art_snippet": "å…ˆè¡ŒæŠ€è¡“ã‹ã‚‰æŠœãå‡ºã—ãŸã€å¯¾å¿œã™ã‚‹æ—¥æœ¬èªã®æ–‡ã¾ãŸã¯ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆåŸæ–‡ã‚’ãã®ã¾ã¾ï¼‰",
      "explanation": "ãªãœã“ã®ãƒšã‚¢ãŒç™ºæ˜ã‚’å¦å®šã™ã‚‹æ ¹æ‹ ã«ãªã‚‹ã®ã‹ã‚’ç°¡æ½”ã«èª¬æ˜ï¼ˆæ—¥æœ¬èªï¼‰"
    }},
    ...
  ]
}}

é‡è¦:
- snippet ã¯å¿…ãšä¸Šè¨˜ã® [æœ¬é¡˜è«‹æ±‚é …å…¨æ–‡] / [å…ˆè¡ŒæŠ€è¡“ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡] ã‹ã‚‰ãã®ã¾ã¾æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
- Markdown ã‚„ HTML ã‚¿ã‚° (<mark> ç­‰) ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""

    response = model.generate_content(prompt)
    try:
        data = json.loads(response.text)
        pairs = data.get("evidence_pairs", [])
    except Exception:
        # LLM ãŒå£Šã‚ŒãŸ JSON ã‚’è¿”ã—ãŸå ´åˆã€ãã‚‚ãã‚‚ JSON ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        pairs = []

    # æŠœãå‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    claim_snippets = [p.get("claim_snippet", "") for p in pairs]
    prior_snippets = [p.get("prior_art_snippet", "") for p in pairs]

    claim_html = "<pre>" + _highlight_snippets(claim_text, claim_snippets) + "</pre>"
    prior_html = "<pre>" + _highlight_snippets(prior_art_text, prior_snippets) + "</pre>"

    # èª¬æ˜ã¯ã²ã¨ã¾ãšå…¨éƒ¨ã¤ãªã’ã‚‹
    explanations = [p.get("explanation", "").strip() for p in pairs if p.get("explanation")]
    explanation_joined = "\n\n".join(explanations)
    reason_merged = (base_reason + "\n\n" + explanation_joined).strip() if explanation_joined else base_reason

    return {
        "claim_html": claim_html,
        "prior_art_html": prior_html,
        "reason": reason_merged,
    }


def run_evidence_extraction_for_doc_numbers(
    query_patent: Patent,
    doc_numbers_to_fetch: list[str],
    ai_judge_results: list[dict],
):
    """
    - å¦å®šã•ã‚ŒãŸæ–‡çŒ® doc_number ã”ã¨ã«
      * æœ¬é¡˜è«‹æ±‚é …ãƒ†ã‚­ã‚¹ãƒˆ
      * prior_art ãƒ†ã‚­ã‚¹ãƒˆ
      * AIå¯©æŸ»ã®ç†ç”±
      ã‚’ã¾ã¨ã‚ã¦ LLM ã«æŠ•ã’ã‚‹
    - çµæœã‚’ DirNames.EVIDENCE_EXTRACTION é…ä¸‹ã« JSON ã§ä¿å­˜
    """

    current_doc_number = str(query_patent.publication.doc_number)
    evidence_extraction_dir = PathManager.get_dir(
        current_doc_number,
        DirNames.EVIDENCE_EXTRACTION,
    )
    evidence_extraction_dir.mkdir(parents=True, exist_ok=True)

    # prior_art ãƒ†ã‚­ã‚¹ãƒˆã¯ generate_reasons() ã§ä½œã£ãŸ retrieved_docs ã‹ã‚‰å–å¾—
    prior_art_text_by_doc: dict[str, str] = {}
    if "retrieved_docs" in st.session_state:
        for d in st.session_state.retrieved_docs:
            prior_art_text_by_doc[str(d["doc_number"])] = d["text"]

    # AIå¯©æŸ»çµæœã‹ã‚‰ã€Œãã® doc_number ã§å¦å®šã•ã‚ŒãŸ claim ã®ç†ç”±ã€ã‚’ã¾ã¨ã‚ã‚‹
    reason_by_doc: dict[str, str] = {}
    for res in ai_judge_results:
        if not isinstance(res, dict):
            continue
        doc_num = str(res.get("prior_art_doc_number", ""))
        if not doc_num or doc_num not in doc_numbers_to_fetch:
            continue
        inv = res.get("inventiveness", {})
        reasons = []
        for claim_name, v in inv.items():
            if not v.get("inventive", True) and v.get("reason"):
                reasons.append(f"{claim_name}: {v['reason']}")
        if reasons:
            reason_by_doc[doc_num] = "\n\n".join(reasons)

    claim_text = query_patent.claims or ""

    # å„æ–‡çŒ®ã«ã¤ã„ã¦ LLM ã‚’å›ã—ã¦ JSON ã‚’ä¿å­˜
    for doc_num in doc_numbers_to_fetch:
        prior_text = prior_art_text_by_doc.get(str(doc_num), "")
        reason_text = reason_by_doc.get(str(doc_num), "")

        if not prior_text:
            # prior_art ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        evidence = _extract_evidence_with_llm(
            claim_text=claim_text,
            prior_art_text=prior_text,
            reason_text=reason_text,
        )

        # page1 ã®æ—¢å­˜è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ã«åˆã‚ã›ã¦ã€ã€Œverified_evidenceã€ã®ãƒªã‚¹ãƒˆã¨ã—ã¦ä¿å­˜
        output_obj = [
            {
                "doc_number": str(doc_num),
                "verified_evidence": [
                    evidence  # {"claim_html", "prior_art_html", "reason"}
                ],
            }
        ]

        out_path = evidence_extraction_dir / f"evidence_{doc_num}.json"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(output_obj, f, ensure_ascii=False, indent=2)



if __name__ == "__main__":
    page_1()