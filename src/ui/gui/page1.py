from pathlib import Path
import json
import pandas as pd
import streamlit as st
from streamlit.runtime.uploaded_file_manager import UploadedFile

# --- æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from infra.config import PROJECT_ROOT, PathManager, DirNames
from model.patent import Patent
from ui.gui import query_detail
from ui.gui import ai_judge_detail
from ui.gui.search_results_list import search_results_list
from ui.gui.prior_art_detail import prior_art_detail
from bigquery.patent_lookup import get_full_patent_info_by_doc_numbers

# å®šæ•°
MAX_CHAR = 300
EXCLUDE_DIRS = {
    DirNames.UPLOADED, DirNames.TOPK, "temp", DirNames.QUERY, DirNames.KNOWLEDGE,
    "__pycache__", ".git", ".ipynb_checkpoints"
}

def reset_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–"""
    keys_to_reset = [
        "df_retrieved", "matched_chunk_markdowns", "reasons",
        "query", "retrieved_docs", "search_results_df",
        "ai_judge_results", "file_content", "project_dir",
        "current_doc_number", "uploaded_dir"
    ]
    for key in keys_to_reset:
        if key in st.session_state:
            del st.session_state[key]

def load_project_by_id(doc_number: str) -> bool:
    """
    ã€å…±é€šå‡¦ç†ã€‘æŒ‡å®šã•ã‚ŒãŸ doc_number ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€SessionStateã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã‚‚ã€æ—¢å­˜é¸æŠæ™‚ã‚‚ã€æœ€çµ‚çš„ã«ã“ã‚Œã‚’å‘¼ã¶ã“ã¨ã§çŠ¶æ…‹ã‚’å¾©å…ƒã™ã‚‹ã€‚
    """
    # 1. ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    reset_session_state()

    try:
        # --- A. åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆXML/Queryï¼‰ã®ãƒ­ãƒ¼ãƒ‰ ---
        uploaded_dir = PathManager.get_uploaded_query_path(doc_number)
        query_file = uploaded_dir / "uploaded_query.txt"

        if not query_file.exists():
            st.error(f"âŒ å‡ºé¡˜ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {query_file}")
            return False

        with open(query_file, "r", encoding="utf-8") as f:
            file_content = f.read()

        # XMLè§£æ
        query: Patent = st.session_state.loader.run(query_file)

        # åŸºæœ¬ã‚¹ãƒ†ãƒ¼ãƒˆè¨­å®š
        st.session_state.file_content = file_content
        st.session_state.query = query
        st.session_state.project_dir = uploaded_dir.parent
        st.session_state.uploaded_dir = uploaded_dir
        st.session_state.current_doc_number = doc_number

        # --- B. æ¤œç´¢çµæœï¼ˆCSVï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        topk_dir = PathManager.get_topk_results_path(doc_number)
        if topk_dir.exists():
            csv_files = sorted(topk_dir.glob("*.csv"))
            if csv_files:
                latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)
                search_results_df = pd.read_csv(latest_csv)
                st.session_state.search_results_df = search_results_df
                st.session_state.df_retrieved = search_results_df
                st.session_state.search_results_csv_path = str(latest_csv)

        # --- C. AIå¯©æŸ»çµæœï¼ˆJSONï¼‰ã®ãƒ­ãƒ¼ãƒ‰ (å­˜åœ¨ã™ã‚Œã°) ---
        ai_judge_dir = PathManager.get_ai_judge_result_path(doc_number)
        if ai_judge_dir.exists():
            json_files = sorted(ai_judge_dir.glob("*.json"))
            if json_files:
                latest_json = json_files[-1]
                with open(latest_json, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                st.session_state.ai_judge_results = results

        return True

    except Exception as e:
        st.error(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ {doc_number} ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.code(traceback.format_exc())
        return False

def handle_new_upload(uploaded_file: UploadedFile):
    """æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®å‡¦ç†ï¼šä¿å­˜ã—ã¦IDã‚’ç‰¹å®šã—ã€å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å‘¼ã¶"""
    try:
        file_content = uploaded_file.read().decode("utf-8")

        # 1. ä¸€æ™‚ä¿å­˜ã—ã¦IDè§£æ (doc_numberã‚’å–å¾—ã™ã‚‹ãŸã‚)
        temp_path = PathManager.get_temp_path("uploaded_query.txt")
        with open(temp_path, "w", encoding="utf-8") as f:
            f.write(file_content)

        with st.spinner("XMLã‚’è§£æä¸­..."):
            query: Patent = st.session_state.loader.run(temp_path)
            doc_number = query.publication.doc_number

            if not doc_number:
                st.error("âŒ XMLã‹ã‚‰ç‰¹è¨±ç•ªå·(doc_number)ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

        # 2. æ­£è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•ãƒ»ä¿å­˜
        PathManager.move_to_permanent(temp_path, doc_number)

        # 3. å…±é€šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ãƒ‰ (ã“ã‚Œã§æ—¢å­˜ãƒ•ãƒ­ãƒ¼ã¨åˆæµ)
        if load_project_by_id(doc_number):
            st.success(f"âœ… æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆãƒ»ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {doc_number}")

    except UnicodeDecodeError:
        st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚UTF-8å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        st.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

def page_1():
    st.title("GENIAC-PRIZE prototype")
    st.subheader("æ±äº¬å¤§å­¦æ¾å°¾å²©æ²¢ç ”ç©¶å®¤ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£")

    mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ("1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "2. æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º"))

    # --- å…¥åŠ›ã‚¨ãƒªã‚¢ã®æç”» ---
    if mode == "1. æ–°è¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.header("ğŸ“ æ–°è¦å‡ºé¡˜ã®å¯©æŸ»")
        uploaded_file = st.file_uploader("1. XMLå½¢å¼ã®å‡ºé¡˜ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xml", "txt"])

        if uploaded_file is not None:
            # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒã€ç¾åœ¨ãƒ­ãƒ¼ãƒ‰ä¸­ã®ã‚‚ã®ã¨é•ã†å ´åˆã®ã¿å‡¦ç†
            # (Streamlitã®ãƒªãƒ­ãƒ¼ãƒ‰å¯¾ç­–)
            current_content = st.session_state.get("file_content")

            # ã¾ã èª­ã¿è¾¼ã‚“ã§ã„ãªã„ã€ã‚ã‚‹ã„ã¯å†…å®¹ãŒå¤‰ã‚ã£ãŸå ´åˆã«å®Ÿè¡Œ
            # æ³¨: uploaded_file.getvalue()ãªã©ã§æ¯”è¼ƒã™ã‚‹æ–¹æ³•ã‚‚ã‚ã‚‹ãŒã€
            # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«æ—¢å­˜stateã®æœ‰ç„¡ã§åˆ¤å®šã—ã€ãƒœã‚¿ãƒ³ãªã—ã§å³æ™‚ãƒ­ãƒ¼ãƒ‰ã•ã›ã‚‹æŒ™å‹•ã‚’ç¶­æŒ
            if not current_content:
                 handle_new_upload(uploaded_file)
            else:
                 # ã™ã§ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ãŸå ´åˆã®æ¤œçŸ¥ã¯
                 # file_uploaderã®keyã‚’å¤‰ãˆã‚‹ã‹ã€IDæ¯”è¼ƒãŒå¿…è¦ã ãŒã€ä»Šå›ã¯ç°¡æ˜“å®Ÿè£…ã¨ã™ã‚‹
                 st.info(f"ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {st.session_state.get('current_doc_number')}")

    else: # æ—¢å­˜æ–‡çŒ®ã®è¡¨ç¤º
        st.header("ğŸ“‚ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‚ç…§")

        eval_dir = PathManager.EVAL_DIR
        if eval_dir.exists():
            projects = [
                d.name for d in eval_dir.iterdir()
                if d.is_dir() and not d.name.startswith('.') and d.name not in EXCLUDE_DIRS
            ]
            projects.sort(reverse=True)

            col1, col2 = st.columns([3, 1])
            with col1:
                selected_doc = st.selectbox("å‡ºé¡˜IDã‚’é¸æŠã—ã¦ãã ã•ã„", projects)
            with col2:
                if st.button("èª­è¾¼", type="primary", width="stretch"):
                    if selected_doc:
                        with st.spinner("ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                            if load_project_by_id(selected_doc):
                                st.success(f"âœ… {selected_doc} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # --- å…±é€šãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢æç”» ---
    # ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    if "query" in st.session_state and st.session_state.get("current_doc_number"):
        st.markdown("---")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŸºæœ¬æƒ…å ±
        with st.expander(f"ğŸ“„ å‡ºé¡˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª: {st.session_state.current_doc_number}"):
            st.text_area("ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«", st.session_state.get("file_content", ""), height=150)

        # Step 2ä»¥é™ã®å…±é€šãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        render_common_steps()


def render_common_steps():
    """
    Step 2ä»¥é™ã®å…±é€šå‡¦ç†
    ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã« st.session_state ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å‰æã§å‹•ä½œã™ã‚‹
    """

    # --- Step 2: é¡ä¼¼æ–‡çŒ®æ¤œç´¢ ---
    st.header("2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢")

    has_search_results = 'search_results_df' in st.session_state and st.session_state.search_results_df is not None

    if has_search_results:
        st.info(f"ï¿½ï¿½ æ¤œç´¢çµæœ: {len(st.session_state.search_results_df):,}ä»¶ å–å¾—æ¸ˆã¿")

        if st.button("ğŸ“‹ è©³ç´°ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º", key="goto_search_list"):
            if "æ¤œç´¢çµæœä¸€è¦§" in st.session_state.page_map:
                st.switch_page(st.session_state.page_map["æ¤œç´¢çµæœä¸€è¦§"])
            else:
                st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: æ¤œç´¢çµæœä¸€è¦§")
        if st.button("ğŸ”„ æ¤œç´¢ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_search"):
            query_detail.query_detail()
    else:
        st.write("Google Patents Public Dataã‚’ç”¨ã„ã¦é¡ä¼¼æ–‡çŒ®ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")
        if st.button("æ¤œç´¢å®Ÿè¡Œ", type="primary", key="run_new_search"):
            query_detail.query_detail()

    # --- Step 3: AIå¯©æŸ» ---
    st.header("3. AIå¯©æŸ»")

    has_ai_results = 'ai_judge_results' in st.session_state and st.session_state.ai_judge_results

    if has_ai_results:
        # æœ‰åŠ¹ãªçµæœã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        valid_results = [r for r in st.session_state.ai_judge_results if r is not None and not (isinstance(r, dict) and 'error' in r)]

        if len(valid_results) == 0:
            st.warning("âš ï¸ AIå¯©æŸ»ã®çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info(f"ğŸ’¾ å¯©æŸ»çµæœ: {len(valid_results)}ä»¶ å–å¾—æ¸ˆã¿")

            with st.expander("å¯©æŸ»çµæœä¸€è¦§ã‚’é–‹ã", expanded=True):
                # DataFrameã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                df_data = []
                valid_indices = []  # æœ‰åŠ¹ãªçµæœã®å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜

                display_idx = 1
                for idx, result in enumerate(st.session_state.ai_judge_results):
                    # result ãŒ None ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if result is None:
                        continue

                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ã‚¹ã‚­ãƒƒãƒ—
                    if isinstance(result, dict) and 'error' in result:
                        continue

                    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’åˆ¤å®š
                    claim_rejected = False
                    if 'inventiveness' in result:
                        for claim in result["inventiveness"]:
                            inventiveness = result["inventiveness"][claim]
                            inventive_bool = inventiveness.get('inventive', True)
                            if not inventive_bool:
                                claim_rejected = True
                                break

                    # å…¬å ±ç•ªå·ã‚’å–å¾—
                    doc_num = result.get('prior_art_doc_number', f"Doc #{display_idx}")

                    # DataFrameã®è¡Œãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    df_data.append({
                        'é †ä½': display_idx,
                        'å…¬å ±ç•ªå·': doc_num,
                        'ç´ä»˜ãå€™è£œã®æœ‰ç„¡': 'æœ‰' if claim_rejected else 'ç„¡'
                    })

                    valid_indices.append(idx)
                    display_idx += 1

                # DataFrameã‚’ä½œæˆã—ã¦è¡¨ç¤º
                if df_data:
                    df = pd.DataFrame(df_data)

                    # ä¿å­˜ç”¨ã®DataFrameã‚’ä½œæˆï¼ˆç´ä»˜ãå€™è£œã®æœ‰ç„¡ã‚’True/Falseã«å¤‰æ›ï¼‰
                    df_to_save = df.copy()
                    df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] = df_to_save['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'].map({'æœ‰': True, 'ç„¡': False})

                    # DataFrameã‚’ä¿å­˜
                    doc_number = st.session_state.current_doc_number
                    save_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")
                    df_to_save.to_csv(save_path, index=False, encoding='utf-8-sig')

                    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name='ai_judge_results.csv',
                        mime='text/csv',
                    )

                    # ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ã«å¿œã˜ã¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨
                    # 10è¡Œã‚’è¶…ãˆã‚‹å ´åˆã®ã¿å›ºå®šé«˜ã•ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã«ã™ã‚‹
                    use_scrollable = len(df_data) > 10
                    container = st.container(height=450) if use_scrollable else st.container()

                    with container:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
                        header_cols = st.columns([1, 3, 2, 2])
                        with header_cols[0]:
                            st.markdown("**é †ä½**")
                        with header_cols[1]:
                            st.markdown("**å…¬å ±ç•ªå·**")
                        with header_cols[2]:
                            st.markdown("**ç´ä»˜ãå€™è£œã®æœ‰ç„¡**")
                        with header_cols[3]:
                            st.markdown("**AIå¯©æŸ»ã®è©³ç´°è¡¨ç¤º**")

                        st.divider()

                        # ãƒ‡ãƒ¼ã‚¿è¡Œ
                        for i, row_data in enumerate(df_data):
                            idx = valid_indices[i]
                            cols = st.columns([1, 3, 2, 2])

                            with cols[0]:
                                st.write(row_data['é †ä½'])
                            with cols[1]:
                                st.write(row_data['å…¬å ±ç•ªå·'])
                            with cols[2]:
                                st.write(row_data['ç´ä»˜ãå€™è£œã®æœ‰ç„¡'])
                            with cols[3]:
                                if st.button("è©³ç´°", key=f"ai_detail_{idx}", use_container_width=True):
                                    st.session_state.selected_prior_art_idx = idx
                                    if "å…ˆè¡ŒæŠ€è¡“è©³ç´°" in st.session_state.page_map:
                                        st.switch_page(st.session_state.page_map["å…ˆè¡ŒæŠ€è¡“è©³ç´°"])
                                    else:
                                        st.error("ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: å…ˆè¡ŒæŠ€è¡“è©³ç´°")

        if st.button("ğŸ”„ AIå¯©æŸ»ã‚’ã‚„ã‚Šç›´ã™", type="primary", key="rerun_ai_judge"):
             run_ai_judge()
    else:
        st.write("LLMã‚’æ´»ç”¨ã—ã€æ–°è¦æ€§ãƒ»é€²æ­©æ€§ã‚’å¯©æŸ»ã—ã¾ã™ã€‚")
        if st.button("AIå¯©æŸ»å®Ÿè¡Œ", type="primary", key="run_ai_judge_new"):
            if not has_search_results:
                st.warning("âš ï¸ å…ˆã«ã€Œ2. é¡ä¼¼æ–‡çŒ®ã®æ¤œç´¢ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                run_ai_judge()

    # ai_judge_results = st.session_state.get("ai_judge_results")
    # if ai_judge_results and type(ai_judge_results) is list :
    #     st.session_state.rejected_df = None
    #     claim_rejected_results = []
        
    #     for ai_result in ai_judge_results:
    #         # doc_number = ai_result["doc_number"]  
    #         # final_decision = ai_result["inventiveness"]

    #         claim_rejected = False 
    #         for claim in ai_result["inventiveness"]:
    #             inventiveness = ai_result["inventiveness"][claim]
    #             inventive_bool = inventiveness['inventive']
    #             if inventive_bool:
    #                 continue
    #             claim_rejected = True
    #         if claim_rejected:
    #             claim_rejected_results.append(ai_result)
    #     if claim_rejected_results:
    #         st.warning(f"ğŸ’¡  å‚ç…§æ–‡çŒ®ã®ç·æ•° (m) = {len(claim_rejected_results)}ä»¶ æ–‡çŒ®ãŒç´ã¥ãã®å€™è£œã®ä»¶æ•°ã€‚")

    #         rejected_dict ={
    #             'doc_number': [r['doc_number'] for r in claim_rejected_results],
    #             'top_k': [r['top_k'] for r in claim_rejected_results],
    #         }
    #         rejected_df = pd.DataFrame(rejected_dict)

    #         # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
    #         st.session_state.rejected_df = rejected_df

    #         st.dataframe(rejected_df)
    #     else:
    #         st.success("âœ… å…¨ã¦ã®è«‹æ±‚é …ã§é€²æ­©æ€§ãŒèªã‚ã‚‰ã‚Œã¾ã—ãŸã€‚")

    # --- Step 4: åˆ¤æ–­æ ¹æ‹ å‡ºåŠ› ---
    st.header("4. åˆ¤æ–­æ ¹æ‹ å‡ºåŠ›")

    if not has_ai_results:
        st.write("âš ï¸ AIå¯©æŸ»ã‚’å®Ÿè¡Œã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
    else:
        ai_judge_results = st.session_state.ai_judge_results

        if st.button("æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ", type="primary"):
            # if "retrieved_docs" not in st.session_state or not st.session_state.retrieved_docs:
            #      st.error("æ–‡çŒ®ãƒ‡ãƒ¼ã‚¿(retrieved_docs)ãŒãƒ¡ãƒ¢ãƒªã«ã‚ã‚Šã¾ã›ã‚“ã€‚å†æ¤œç´¢ãŒå¿…è¦ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            # else:
            generate_reasons(ai_judge_results)

        if "reasons" in st.session_state and st.session_state.reasons:
            for i, reason in enumerate(st.session_state.reasons):
                st.markdown(f"##### åˆ¤æ–­æ ¹æ‹  {i + 1}")
                st.code(reason, language="markdown")


def run_ai_judge():
    """AIå¯©æŸ»å®Ÿè¡Œãƒ©ãƒƒãƒ‘ãƒ¼"""
    st.session_state.n_topk = len(st.session_state.df_retrieved)
    with st.spinner("å¯©æŸ»ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œä¸­..."):
        results = ai_judge_detail.entry(action="button_click")
        if results:
            st.session_state.ai_judge_results = results
            st.success("âœ… AIå¯©æŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()

def generate_reasons(ai_judge_results):
    """æ ¹æ‹ ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯"""
    # query_object = st.session_state.query
    # rejected_dfã‚’ï¼™ä»¶ã¾ã§è¡¨ç¤ºã™ã‚‹
    #ï¼™ä»¶ã«æº€ãŸãªã„å ´åˆã¯ã€top_kã‹ã‚‰ä¸è¶³ã—ã¦ã„ã‚‹åˆ†ã‚’è£œå®Œã™ã‚‹
    competition_rule_max_m = 9
    print(competition_rule_max_m, ": mMaxã®è¨­å®š")

    # eval/{doc_number}/ai_judge_result_tableã‹ã‚‰csvã‚’èª­ã¿è¾¼ã¿
    doc_number = st.session_state.current_doc_number
    csv_path = PathManager.get_file(doc_number, DirNames.AI_JUDGE_TABLE, "ai_judge_table.csv")

    if not csv_path.exists():
        st.error(f"âŒ AIå¯©æŸ»çµæœãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return

    # CSVã‚’èª­ã¿è¾¼ã¿
    df_ai_judge = pd.read_csv(csv_path, encoding='utf-8-sig')

    # ç´ä»˜ãå€™è£œã®æœ‰ç„¡_boolãŒTrueã®ã‚‚ã®ã‚’æŠ½å‡º
    rejected_df = df_ai_judge[df_ai_judge['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool'] == True].copy()

    if len(rejected_df) == 0:
        st.info("âœ… ç´ä»˜ãå€™è£œãŒã‚ã‚‹æ–‡çŒ®ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # rejected_dfã«doc_numberåˆ—ã¨top_kåˆ—ã‚’è¿½åŠ 
    rejected_df['doc_number'] = rejected_df['å…¬å ±ç•ªå·']
    rejected_df['reject_document_exists'] = rejected_df['ç´ä»˜ãå€™è£œã®æœ‰ç„¡_bool']

    actual_limit = min(competition_rule_max_m, len(rejected_df))

    # rejected_dfã‹ã‚‰å…¨ã¦ã®doc_numberã‚’å–å¾—
    doc_numbers_to_fetch = rejected_df.head(actual_limit)['doc_number'].tolist()
    reject_document_exists_list = rejected_df.head(actual_limit)['reject_document_exists'].tolist() 
    # reject_document_exists_listãŒTrueã®ã‚‚ã®ã ã‘ã«çµã‚‹
    doc_numbers_to_fetch = [doc_num for doc_num, exists in zip(doc_numbers_to_fetch, reject_document_exists_list) if exists]    

    # BigQueryã‹ã‚‰ä¸€æ‹¬ã§ç‰¹è¨±æƒ…å ±ã‚’å–å¾—
    with st.spinner("BigQueryã‹ã‚‰ç‰¹è¨±æƒ…å ±ã‚’å–å¾—ä¸­..."):
        get_full_patent_info_by_doc_numbers(doc_numbers_to_fetch, doc_number)


    # doc_numberã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã«å¤‰æ›ï¼ˆé«˜é€Ÿæ¤œç´¢ã®ãŸã‚ï¼‰
    patent_info_dict = {info['doc_number']: info for info in patent_info_list}

    # retrieved_docsã«ç‰¹è¨±æƒ…å ±ã‚’è¿½åŠ ã¾ãŸã¯æ›´æ–°
    if "retrieved_docs" not in st.session_state:
        st.session_state.retrieved_docs = []

    for i, target_row in rejected_df.head(actual_limit).iterrows():
        doc_number = target_row['doc_number']

        # å¯¾å¿œã™ã‚‹retrieved_docsã‚’æ¢ã™
        doc_found = False
        for doc in st.session_state.retrieved_docs:
            if doc.get('doc_number') == doc_number:
                # æ—¢å­˜ã®docã«BigQueryã‹ã‚‰å–å¾—ã—ãŸæƒ…å ±ã‚’è¿½åŠ 
                if doc_number in patent_info_dict:
                    patent_info = patent_info_dict[doc_number]
                    doc['title'] = patent_info['title']
                    doc['abstract'] = patent_info['abstract']
                    doc['claims'] = patent_info['claims']
                    doc['description'] = patent_info['description']
                doc_found = True
                break

        # è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ã€æ–°è¦ã«docã‚’ä½œæˆ
        if not doc_found and doc_number in patent_info_dict:
            patent_info = patent_info_dict[doc_number]
            new_doc = {
                'doc_number': doc_number,
                'title': patent_info['title'],
                'abstract': patent_info['abstract'],
                'claims': patent_info['claims'],
                'description': patent_info['description']
            }
            st.session_state.retrieved_docs.append(new_doc)

    st.success(f"âœ… {len(patent_info_list)}ä»¶ã®ç‰¹è¨±æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")




    st.session_state.reasons = []
    status_text = st.empty()
    progress = st.progress(0)
    final_decision = ai_judge_results[0]["final_decision"] 
    conversation_history = ai_judge_results[0]["conversation_history"] 
    inventiveness_keys = dict(ai_judge_results[0]["inventiveness"]).keys()
    for key in inventiveness_keys:
        if key.startswith('claim'):
            st.session_state.query.claims.append(key.upper())

     # å‹•ä½œç¢ºèªç”¨ãƒ€ãƒŸãƒ¼ã‚¢ã‚¯ã‚»ã‚¹
    (['doc_number', 'top_k', 'application_structure', 'prior_art_structure', 'applicant_arguments', 'examiner_review', 'final_decision', 'conversation_history', 'inventiveness', 'prior_art_doc_number'])

    for i in range(actual_limit):
        status_text.text(f"{i + 1} / {actual_limit} ä»¶ç›®ã‚’ç”Ÿæˆä¸­ã§ã™...")
        if "generator" in st.session_state:
            reason = st.session_state.generator.generate(
                st.session_state.query,
                st.session_state.retrieved_docs[i]
            )
            st.session_state.reasons.append(reason)
        else:
            st.error("GeneratorãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            break
        progress.progress((i + 1) / actual_limit)

    status_text.text("ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    page_1()